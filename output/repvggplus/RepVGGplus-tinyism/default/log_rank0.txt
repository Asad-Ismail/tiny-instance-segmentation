[2022-12-19 20:31:24 RepVGGplus-tinyism] (train_repvgg.py 258): INFO Full config saved to ./output/repvggplus/RepVGGplus-tinyism/default/config.json
[2022-12-19 20:31:24 RepVGGplus-tinyism] (train_repvgg.py 261): INFO AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.2
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  PRESET: null
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 8
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: hub://aismail2/cucumber_OD
  IMG_SIZE: 224
  INTERPOLATION: bilinear
  NUM_WORKERS: 8
  PIN_MEMORY: true
  TEST_BATCH_SIZE: 8
  TEST_SIZE: 224
  ZIP_MODE: false
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  ARCH: RepVGGplus-tinyism
  LABEL_SMOOTHING: 0.1
  NUM_CLASSES: 1000
  RESUME: ''
OUTPUT: ./output/repvggplus/RepVGGplus-tinyism/default
PRINT_FREQ: 10
SAVE_FREQ: 20
SEED: 0
TAG: default
TEST:
  CROP: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 0.0
  EMA_ALPHA: 0.0
  EMA_UPDATE_PERIOD: 8
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 0.0
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  SCALES_PATH: null
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 0.0
  WEIGHT_DECAY: 0.05

[2022-12-19 20:31:24 RepVGGplus-tinyism] (train_repvgg.py 66): INFO Creating model:RepVGGplus-tinyism
[2022-12-19 20:31:52 RepVGGplus-tinyism] (train_repvgg.py 73): INFO tinyModel(
  (backbone): RepVGGplusPhen(
    (stage0): RepVGGplusBlock(
      (nonlinearity): ReLU()
      (post_se): SEBlock(
        (down): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (up): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))
      )
      (rbr_dense): Sequential(
        (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (rbr_1x1): Sequential(
        (conv): Conv2d(3, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (stage1): RepVGGplusStage(
      (blocks): ModuleList(
        (0): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(160, 40, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(40, 160, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_dense): Sequential(
            (conv): Conv2d(64, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(64, 160, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(160, 40, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(40, 160, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (2): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(160, 40, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(40, 160, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (3): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(160, 40, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(40, 160, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (4): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(160, 40, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(40, 160, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (5): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(160, 40, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(40, 160, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (6): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(160, 40, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(40, 160, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (7): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(160, 40, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(40, 160, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (stage2): RepVGGplusStage(
      (blocks): ModuleList(
        (0): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_dense): Sequential(
            (conv): Conv2d(160, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(160, 320, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (2): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (3): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (4): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (5): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (6): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (7): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (8): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (9): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (10): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (11): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (12): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (13): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (stage3): RepVGGplusStage(
      (blocks): ModuleList(
        (0): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_dense): Sequential(
            (conv): Conv2d(320, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(320, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (2): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (3): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (4): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (5): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (6): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (7): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (8): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (9): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (10): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (11): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (12): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (13): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (14): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (15): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (16): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (17): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (18): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (19): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (20): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (21): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (22): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (23): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
    )
  )
  (class_head): Sequential(
    (0): Conv2d(514, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (conv_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_0): ReLU()
    (conv_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_1): ReLU()
    (conv_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_2): ReLU()
    (conv_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_3): ReLU()
    (conv_4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_4): ReLU()
    (conv_5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_5): ReLU()
    (cls): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))
  )
  (box_head): Sequential(
    (0): Conv2d(514, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (conv_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_0): ReLU()
    (conv_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_1): ReLU()
    (conv_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_2): ReLU()
    (conv_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_3): ReLU()
    (conv_4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_4): ReLU()
    (conv_5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_5): ReLU()
  )
  (box): Conv2d(256, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (offset_head): Sequential(
    (0): Conv2d(514, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (conv_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_0): ReLU()
    (conv_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_1): ReLU()
    (conv_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_2): ReLU()
    (conv_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_3): ReLU()
    (conv_4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_4): ReLU()
    (conv_5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_5): ReLU()
  )
  (offset): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))
  (seg_head): Sequential(
    (0): Conv2d(5, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (conv_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_0): ReLU()
    (conv_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_1): ReLU()
    (conv_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_2): ReLU()
    (conv_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_3): ReLU()
    (conv_4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_4): ReLU()
    (conv_5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_5): ReLU()
  )
  (seg): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))
)
[2022-12-19 20:31:53 RepVGGplus-tinyism] (train_repvgg.py 88): INFO number of params: 99485497
[2022-12-19 20:31:53 RepVGGplus-tinyism] (train_repvgg.py 104): INFO Start training
[2022-12-19 20:31:59 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [0/300][0/37]	eta 0:03:35 lr 0.000000	time 5.8166 (5.8166)	loss 209.6932 (209.6932)	grad_norm 18.8662 (18.8662)	mem 14374MB
[2022-12-19 20:41:46 RepVGGplus-tinyism] (train_repvgg.py 258): INFO Full config saved to ./output/repvggplus/RepVGGplus-tinyism/default/config.json
[2022-12-19 20:41:46 RepVGGplus-tinyism] (train_repvgg.py 261): INFO AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.2
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  PRESET: null
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 8
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: hub://aismail2/cucumber_OD
  IMG_SIZE: 224
  INTERPOLATION: bilinear
  NUM_WORKERS: 8
  PIN_MEMORY: true
  TEST_BATCH_SIZE: 8
  TEST_SIZE: 224
  ZIP_MODE: false
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  ARCH: RepVGGplus-tinyism
  LABEL_SMOOTHING: 0.1
  NUM_CLASSES: 1000
  RESUME: ''
OUTPUT: ./output/repvggplus/RepVGGplus-tinyism/default
PRINT_FREQ: 10
SAVE_FREQ: 20
SEED: 0
TAG: default
TEST:
  CROP: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 1.5625e-05
  CLIP_GRAD: 0.0
  EMA_ALPHA: 0.0
  EMA_UPDATE_PERIOD: 8
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 0.0
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  SCALES_PATH: null
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 0.0
  WEIGHT_DECAY: 0.05

[2022-12-19 20:41:46 RepVGGplus-tinyism] (train_repvgg.py 66): INFO Creating model:RepVGGplus-tinyism
[2022-12-19 20:42:13 RepVGGplus-tinyism] (train_repvgg.py 73): INFO tinyModel(
  (backbone): RepVGGplusPhen(
    (stage0): RepVGGplusBlock(
      (nonlinearity): ReLU()
      (post_se): SEBlock(
        (down): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (up): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))
      )
      (rbr_dense): Sequential(
        (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (rbr_1x1): Sequential(
        (conv): Conv2d(3, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (stage1): RepVGGplusStage(
      (blocks): ModuleList(
        (0): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(160, 40, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(40, 160, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_dense): Sequential(
            (conv): Conv2d(64, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(64, 160, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(160, 40, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(40, 160, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (2): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(160, 40, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(40, 160, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (3): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(160, 40, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(40, 160, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (4): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(160, 40, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(40, 160, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (5): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(160, 40, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(40, 160, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (6): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(160, 40, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(40, 160, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (7): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(160, 40, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(40, 160, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (stage2): RepVGGplusStage(
      (blocks): ModuleList(
        (0): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_dense): Sequential(
            (conv): Conv2d(160, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(160, 320, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (2): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (3): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (4): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (5): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (6): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (7): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (8): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (9): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (10): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (11): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (12): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (13): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (stage3): RepVGGplusStage(
      (blocks): ModuleList(
        (0): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_dense): Sequential(
            (conv): Conv2d(320, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(320, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (2): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (3): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (4): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (5): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (6): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (7): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (8): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (9): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (10): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (11): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (12): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (13): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (14): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (15): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (16): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (17): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (18): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (19): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (20): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (21): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (22): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (23): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
    )
  )
  (class_head): Sequential(
    (0): Conv2d(514, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (conv_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_0): ReLU()
    (conv_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_1): ReLU()
    (conv_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_2): ReLU()
    (conv_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_3): ReLU()
    (conv_4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_4): ReLU()
    (conv_5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_5): ReLU()
    (cls): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))
  )
  (box_head): Sequential(
    (0): Conv2d(514, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (conv_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_0): ReLU()
    (conv_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_1): ReLU()
    (conv_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_2): ReLU()
    (conv_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_3): ReLU()
    (conv_4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_4): ReLU()
    (conv_5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_5): ReLU()
  )
  (box): Conv2d(256, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (offset_head): Sequential(
    (0): Conv2d(514, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (conv_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_0): ReLU()
    (conv_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_1): ReLU()
    (conv_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_2): ReLU()
    (conv_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_3): ReLU()
    (conv_4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_4): ReLU()
    (conv_5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_5): ReLU()
  )
  (offset): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))
  (seg_head): Sequential(
    (0): Conv2d(5, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (conv_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_0): ReLU()
    (conv_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_1): ReLU()
    (conv_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_2): ReLU()
    (conv_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_3): ReLU()
    (conv_4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_4): ReLU()
    (conv_5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_5): ReLU()
  )
  (seg): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))
)
[2022-12-19 20:42:15 RepVGGplus-tinyism] (train_repvgg.py 88): INFO number of params: 99485497
[2022-12-19 20:42:15 RepVGGplus-tinyism] (train_repvgg.py 104): INFO Start training
[2022-12-19 20:42:21 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [0/300][0/37]	eta 0:03:37 lr 0.000000	time 5.8735 (5.8735)	loss 209.6932 (209.6932)	grad_norm 18.8744 (18.8744)	mem 14374MB
[2022-12-19 20:42:40 RepVGGplus-tinyism] (train_repvgg.py 258): INFO Full config saved to ./output/repvggplus/RepVGGplus-tinyism/default/config.json
[2022-12-19 20:42:40 RepVGGplus-tinyism] (train_repvgg.py 261): INFO AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.2
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  PRESET: null
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 4
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: hub://aismail2/cucumber_OD
  IMG_SIZE: 224
  INTERPOLATION: bilinear
  NUM_WORKERS: 8
  PIN_MEMORY: true
  TEST_BATCH_SIZE: 4
  TEST_SIZE: 224
  ZIP_MODE: false
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  ARCH: RepVGGplus-tinyism
  LABEL_SMOOTHING: 0.1
  NUM_CLASSES: 1000
  RESUME: ''
OUTPUT: ./output/repvggplus/RepVGGplus-tinyism/default
PRINT_FREQ: 10
SAVE_FREQ: 20
SEED: 0
TAG: default
TEST:
  CROP: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 7.8125e-06
  CLIP_GRAD: 0.0
  EMA_ALPHA: 0.0
  EMA_UPDATE_PERIOD: 8
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 0.0
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  SCALES_PATH: null
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 0.0
  WEIGHT_DECAY: 0.05

[2022-12-19 20:42:40 RepVGGplus-tinyism] (train_repvgg.py 66): INFO Creating model:RepVGGplus-tinyism
[2022-12-19 20:43:05 RepVGGplus-tinyism] (train_repvgg.py 73): INFO tinyModel(
  (backbone): RepVGGplusPhen(
    (stage0): RepVGGplusBlock(
      (nonlinearity): ReLU()
      (post_se): SEBlock(
        (down): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (up): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))
      )
      (rbr_dense): Sequential(
        (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (rbr_1x1): Sequential(
        (conv): Conv2d(3, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (stage1): RepVGGplusStage(
      (blocks): ModuleList(
        (0): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(160, 40, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(40, 160, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_dense): Sequential(
            (conv): Conv2d(64, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(64, 160, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(160, 40, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(40, 160, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (2): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(160, 40, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(40, 160, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (3): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(160, 40, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(40, 160, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (4): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(160, 40, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(40, 160, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (5): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(160, 40, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(40, 160, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (6): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(160, 40, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(40, 160, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (7): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(160, 40, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(40, 160, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (stage2): RepVGGplusStage(
      (blocks): ModuleList(
        (0): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_dense): Sequential(
            (conv): Conv2d(160, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(160, 320, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (2): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (3): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (4): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (5): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (6): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (7): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (8): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (9): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (10): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (11): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (12): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (13): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (stage3): RepVGGplusStage(
      (blocks): ModuleList(
        (0): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_dense): Sequential(
            (conv): Conv2d(320, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(320, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (2): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (3): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (4): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (5): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (6): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (7): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (8): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (9): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (10): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (11): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (12): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (13): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (14): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (15): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (16): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (17): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (18): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (19): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (20): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (21): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (22): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (23): RepVGGplusBlock(
          (nonlinearity): ReLU()
          (post_se): SEBlock(
            (down): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (up): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (rbr_identity): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (rbr_dense): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (rbr_1x1): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
    )
  )
  (class_head): Sequential(
    (0): Conv2d(514, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (conv_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_0): ReLU()
    (conv_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_1): ReLU()
    (conv_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_2): ReLU()
    (conv_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_3): ReLU()
    (conv_4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_4): ReLU()
    (conv_5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_5): ReLU()
    (cls): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))
  )
  (box_head): Sequential(
    (0): Conv2d(514, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (conv_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_0): ReLU()
    (conv_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_1): ReLU()
    (conv_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_2): ReLU()
    (conv_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_3): ReLU()
    (conv_4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_4): ReLU()
    (conv_5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_5): ReLU()
  )
  (box): Conv2d(256, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (offset_head): Sequential(
    (0): Conv2d(514, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (conv_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_0): ReLU()
    (conv_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_1): ReLU()
    (conv_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_2): ReLU()
    (conv_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_3): ReLU()
    (conv_4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_4): ReLU()
    (conv_5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_5): ReLU()
  )
  (offset): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))
  (seg_head): Sequential(
    (0): Conv2d(5, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (conv_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_0): ReLU()
    (conv_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_1): ReLU()
    (conv_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_2): ReLU()
    (conv_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_3): ReLU()
    (conv_4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_4): ReLU()
    (conv_5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu_5): ReLU()
  )
  (seg): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))
)
[2022-12-19 20:43:07 RepVGGplus-tinyism] (train_repvgg.py 88): INFO number of params: 99485497
[2022-12-19 20:43:07 RepVGGplus-tinyism] (train_repvgg.py 104): INFO Start training
[2022-12-19 20:43:11 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [0/300][0/74]	eta 0:05:05 lr 0.000000	time 4.1318 (4.1318)	loss 277.9577 (277.9577)	grad_norm 26.9403 (26.9403)	mem 14369MB
[2022-12-19 20:43:20 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [0/300][10/74]	eta 0:01:16 lr 0.000000	time 0.9505 (1.1950)	loss 243.5136 (265.7517)	grad_norm 22.6631 (24.0353)	mem 14369MB
[2022-12-19 20:43:29 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [0/300][20/74]	eta 0:00:56 lr 0.000000	time 0.8550 (1.0554)	loss 257.5560 (257.2241)	grad_norm 28.8026 (24.4359)	mem 14369MB
[2022-12-19 20:43:38 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [0/300][30/74]	eta 0:00:44 lr 0.000000	time 0.8550 (1.0005)	loss 210.5063 (242.4778)	grad_norm 30.6772 (24.5776)	mem 14369MB
[2022-12-19 20:43:46 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [0/300][40/74]	eta 0:00:32 lr 0.000000	time 0.8535 (0.9659)	loss 267.4217 (240.4779)	grad_norm 14.5305 (24.2793)	mem 14369MB
[2022-12-19 20:43:55 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [0/300][50/74]	eta 0:00:22 lr 0.000000	time 0.8560 (0.9470)	loss 186.2683 (241.0107)	grad_norm 24.7541 (23.6444)	mem 14369MB
[2022-12-19 20:44:04 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [0/300][60/74]	eta 0:00:13 lr 0.000000	time 0.8494 (0.9319)	loss 249.7673 (243.8451)	grad_norm 16.6329 (23.5086)	mem 14369MB
[2022-12-19 20:44:12 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [0/300][70/74]	eta 0:00:03 lr 0.000000	time 0.8494 (0.9205)	loss 259.2487 (245.7738)	grad_norm 26.7280 (23.6454)	mem 14369MB
[2022-12-19 20:44:16 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 0 training takes 0:01:08
[2022-12-19 20:44:16 RepVGGplus-tinyism] (helpers.py 207): INFO ./output/repvggplus/RepVGGplus-tinyism/default/ckpt_epoch_0.pth saving......
[2022-12-19 20:44:17 RepVGGplus-tinyism] (helpers.py 209): INFO ./output/repvggplus/RepVGGplus-tinyism/default/ckpt_epoch_0.pth saved !!!
[2022-12-19 20:44:18 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [1/300][0/74]	eta 0:01:51 lr 0.000000	time 1.5001 (1.5001)	loss 239.8614 (239.8614)	grad_norm 24.9697 (24.9697)	mem 14369MB
[2022-12-19 20:44:27 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [1/300][10/74]	eta 0:00:58 lr 0.000000	time 0.8563 (0.9164)	loss 227.2240 (230.0052)	grad_norm 32.8468 (27.7731)	mem 14369MB
[2022-12-19 20:44:35 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [1/300][20/74]	eta 0:00:47 lr 0.000000	time 0.8630 (0.8885)	loss 250.2433 (235.6797)	grad_norm 35.7949 (26.3640)	mem 14369MB
[2022-12-19 20:44:44 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [1/300][30/74]	eta 0:00:38 lr 0.000001	time 0.8548 (0.8777)	loss 247.1858 (242.8040)	grad_norm 21.3078 (23.8438)	mem 14369MB
[2022-12-19 20:44:53 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [1/300][40/74]	eta 0:00:29 lr 0.000001	time 0.8533 (0.8727)	loss 261.7067 (240.6149)	grad_norm 28.5682 (23.6472)	mem 14369MB
[2022-12-19 20:45:01 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [1/300][50/74]	eta 0:00:20 lr 0.000001	time 0.8708 (0.8706)	loss 171.3956 (239.3163)	grad_norm 16.6367 (22.9563)	mem 14369MB
[2022-12-19 20:45:10 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [1/300][60/74]	eta 0:00:12 lr 0.000001	time 0.8625 (0.8686)	loss 240.1194 (241.9460)	grad_norm 37.7798 (23.1735)	mem 14369MB
[2022-12-19 20:45:18 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [1/300][70/74]	eta 0:00:03 lr 0.000001	time 0.8740 (0.8677)	loss 192.8998 (242.5929)	grad_norm 12.4143 (23.8040)	mem 14369MB
[2022-12-19 20:45:21 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 1 training takes 0:01:04
[2022-12-19 20:45:22 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [2/300][0/74]	eta 0:01:51 lr 0.000001	time 1.5051 (1.5051)	loss 208.5481 (208.5481)	grad_norm 13.2140 (13.2140)	mem 14369MB
[2022-12-19 20:45:31 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [2/300][10/74]	eta 0:00:58 lr 0.000001	time 0.8561 (0.9202)	loss 257.5910 (234.1871)	grad_norm 37.9238 (20.8804)	mem 14369MB
[2022-12-19 20:45:40 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [2/300][20/74]	eta 0:00:48 lr 0.000001	time 0.8626 (0.8941)	loss 233.8179 (233.8173)	grad_norm 22.8892 (20.8884)	mem 14369MB
[2022-12-19 20:45:48 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [2/300][30/74]	eta 0:00:38 lr 0.000001	time 0.8651 (0.8845)	loss 331.1350 (247.3702)	grad_norm 27.6549 (22.8121)	mem 14369MB
[2022-12-19 20:45:57 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [2/300][40/74]	eta 0:00:29 lr 0.000001	time 0.8648 (0.8814)	loss 192.1312 (247.3195)	grad_norm 31.1641 (22.8765)	mem 14369MB
[2022-12-19 20:46:06 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [2/300][50/74]	eta 0:00:21 lr 0.000001	time 0.8616 (0.8789)	loss 226.9348 (245.8666)	grad_norm 29.1996 (23.8968)	mem 14369MB
[2022-12-19 20:46:14 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [2/300][60/74]	eta 0:00:12 lr 0.000001	time 0.8735 (0.8775)	loss 203.4650 (245.4321)	grad_norm 29.1504 (23.3521)	mem 14369MB
[2022-12-19 20:46:23 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [2/300][70/74]	eta 0:00:03 lr 0.000001	time 0.8671 (0.8768)	loss 324.6681 (243.9808)	grad_norm 19.8742 (23.2208)	mem 14369MB
[2022-12-19 20:46:26 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 2 training takes 0:01:04
[2022-12-19 20:46:27 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [3/300][0/74]	eta 0:01:54 lr 0.000001	time 1.5483 (1.5483)	loss 194.2690 (194.2690)	grad_norm 13.3935 (13.3935)	mem 14369MB
[2022-12-19 20:46:36 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [3/300][10/74]	eta 0:00:59 lr 0.000001	time 0.8708 (0.9320)	loss 233.4409 (235.7220)	grad_norm 20.7225 (22.1879)	mem 14369MB
[2022-12-19 20:46:45 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [3/300][20/74]	eta 0:00:48 lr 0.000001	time 0.8784 (0.9055)	loss 215.9071 (234.1284)	grad_norm 13.1535 (22.6405)	mem 14369MB
[2022-12-19 20:46:53 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [3/300][30/74]	eta 0:00:39 lr 0.000001	time 0.8828 (0.8954)	loss 184.0253 (237.2252)	grad_norm 13.4865 (21.8130)	mem 14369MB
[2022-12-19 20:47:02 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [3/300][40/74]	eta 0:00:30 lr 0.000001	time 0.8713 (0.8895)	loss 246.7925 (239.3168)	grad_norm 34.0243 (22.6488)	mem 14369MB
[2022-12-19 20:47:11 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [3/300][50/74]	eta 0:00:21 lr 0.000001	time 0.8642 (0.8862)	loss 271.4241 (237.3724)	grad_norm 20.3442 (22.5507)	mem 14369MB
[2022-12-19 20:47:20 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [3/300][60/74]	eta 0:00:12 lr 0.000001	time 0.8747 (0.8833)	loss 217.4444 (242.3526)	grad_norm 44.3199 (22.9971)	mem 14369MB
[2022-12-19 20:47:28 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [3/300][70/74]	eta 0:00:03 lr 0.000002	time 0.8638 (0.8812)	loss 256.5298 (242.5810)	grad_norm 15.7419 (23.0175)	mem 14369MB
[2022-12-19 20:47:31 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 3 training takes 0:01:05
[2022-12-19 20:47:32 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [4/300][0/74]	eta 0:01:53 lr 0.000002	time 1.5320 (1.5320)	loss 232.8031 (232.8031)	grad_norm 40.8374 (40.8374)	mem 14369MB
[2022-12-19 20:47:41 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [4/300][10/74]	eta 0:00:59 lr 0.000002	time 0.8621 (0.9281)	loss 260.0429 (234.3726)	grad_norm 16.8438 (24.5936)	mem 14369MB
[2022-12-19 20:47:50 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [4/300][20/74]	eta 0:00:48 lr 0.000002	time 0.8641 (0.9014)	loss 310.5063 (241.1627)	grad_norm 16.0806 (22.6091)	mem 14369MB
[2022-12-19 20:47:58 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [4/300][30/74]	eta 0:00:39 lr 0.000002	time 0.8654 (0.8905)	loss 220.1889 (240.5543)	grad_norm 35.7997 (22.9229)	mem 14369MB
[2022-12-19 20:48:07 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [4/300][40/74]	eta 0:00:30 lr 0.000002	time 0.8874 (0.8856)	loss 172.4372 (240.8016)	grad_norm 18.0625 (23.4353)	mem 14369MB
[2022-12-19 20:48:16 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [4/300][50/74]	eta 0:00:21 lr 0.000002	time 0.8667 (0.8839)	loss 281.5207 (241.1384)	grad_norm 22.3293 (22.7106)	mem 14369MB
[2022-12-19 20:48:25 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [4/300][60/74]	eta 0:00:12 lr 0.000002	time 0.8612 (0.8817)	loss 309.0002 (242.5881)	grad_norm 36.2120 (23.4600)	mem 14369MB
[2022-12-19 20:48:33 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [4/300][70/74]	eta 0:00:03 lr 0.000002	time 0.8621 (0.8803)	loss 338.4733 (243.0051)	grad_norm 16.7243 (23.2849)	mem 14369MB
[2022-12-19 20:48:36 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 4 training takes 0:01:04
[2022-12-19 20:48:37 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [5/300][0/74]	eta 0:01:52 lr 0.000002	time 1.5254 (1.5254)	loss 274.1748 (274.1748)	grad_norm 26.1949 (26.1949)	mem 14369MB
[2022-12-19 20:48:46 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [5/300][10/74]	eta 0:00:59 lr 0.000002	time 0.8861 (0.9307)	loss 189.7261 (253.9205)	grad_norm 17.0387 (21.6523)	mem 14369MB
[2022-12-19 20:48:55 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [5/300][20/74]	eta 0:00:48 lr 0.000002	time 0.8702 (0.9051)	loss 230.0672 (238.9777)	grad_norm 30.6329 (20.5487)	mem 14369MB
[2022-12-19 20:49:04 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [5/300][30/74]	eta 0:00:39 lr 0.000002	time 0.8698 (0.8935)	loss 221.9001 (241.4928)	grad_norm 19.4610 (21.4530)	mem 14369MB
[2022-12-19 20:49:12 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [5/300][40/74]	eta 0:00:30 lr 0.000002	time 0.8851 (0.8894)	loss 202.2792 (240.8118)	grad_norm 12.9157 (21.7989)	mem 14369MB
[2022-12-19 20:49:21 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [5/300][50/74]	eta 0:00:21 lr 0.000002	time 0.8783 (0.8868)	loss 188.8194 (241.5579)	grad_norm 13.7974 (22.1485)	mem 14369MB
[2022-12-19 20:49:30 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [5/300][60/74]	eta 0:00:12 lr 0.000002	time 0.8677 (0.8843)	loss 343.4555 (243.5094)	grad_norm 28.8399 (22.2328)	mem 14369MB
[2022-12-19 20:49:38 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [5/300][70/74]	eta 0:00:03 lr 0.000002	time 0.8760 (0.8825)	loss 238.2918 (241.8605)	grad_norm 18.4496 (22.4022)	mem 14369MB
[2022-12-19 20:49:41 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 5 training takes 0:01:05
[2022-12-19 20:49:43 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [6/300][0/74]	eta 0:01:54 lr 0.000002	time 1.5450 (1.5450)	loss 287.7211 (287.7211)	grad_norm 16.2300 (16.2300)	mem 14369MB
[2022-12-19 20:49:51 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [6/300][10/74]	eta 0:00:59 lr 0.000002	time 0.8628 (0.9286)	loss 334.8525 (257.4686)	grad_norm 20.4762 (24.6361)	mem 14369MB
[2022-12-19 20:50:00 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [6/300][20/74]	eta 0:00:48 lr 0.000002	time 0.8631 (0.9000)	loss 264.1977 (250.9331)	grad_norm 14.4953 (24.6971)	mem 14369MB
[2022-12-19 20:50:09 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [6/300][30/74]	eta 0:00:39 lr 0.000003	time 0.8648 (0.8896)	loss 284.3126 (257.8713)	grad_norm 16.7064 (23.4644)	mem 14369MB
[2022-12-19 20:50:17 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [6/300][40/74]	eta 0:00:30 lr 0.000003	time 0.8652 (0.8858)	loss 255.3421 (252.3044)	grad_norm 22.6775 (22.8881)	mem 14369MB
[2022-12-19 20:50:26 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [6/300][50/74]	eta 0:00:21 lr 0.000003	time 0.8670 (0.8834)	loss 304.0375 (247.0816)	grad_norm 18.2364 (23.0431)	mem 14369MB
[2022-12-19 20:50:35 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [6/300][60/74]	eta 0:00:12 lr 0.000003	time 0.8671 (0.8810)	loss 249.5287 (248.3241)	grad_norm 36.8401 (23.5530)	mem 14369MB
[2022-12-19 20:50:44 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [6/300][70/74]	eta 0:00:03 lr 0.000003	time 0.8743 (0.8808)	loss 260.5728 (245.1410)	grad_norm 23.9342 (23.0686)	mem 14369MB
[2022-12-19 20:50:46 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 6 training takes 0:01:05
[2022-12-19 20:50:48 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [7/300][0/74]	eta 0:01:54 lr 0.000003	time 1.5432 (1.5432)	loss 258.2765 (258.2765)	grad_norm 15.9195 (15.9195)	mem 14369MB
[2022-12-19 20:50:56 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [7/300][10/74]	eta 0:00:59 lr 0.000003	time 0.8729 (0.9331)	loss 220.6587 (239.4082)	grad_norm 16.6431 (22.3604)	mem 14369MB
[2022-12-19 20:51:05 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [7/300][20/74]	eta 0:00:48 lr 0.000003	time 0.8708 (0.9047)	loss 262.5104 (237.3566)	grad_norm 17.6005 (21.0545)	mem 14369MB
[2022-12-19 20:51:14 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [7/300][30/74]	eta 0:00:39 lr 0.000003	time 0.8757 (0.8937)	loss 211.8176 (240.4717)	grad_norm 27.8210 (23.2656)	mem 14369MB
[2022-12-19 20:51:23 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [7/300][40/74]	eta 0:00:30 lr 0.000003	time 0.8700 (0.8888)	loss 314.8737 (240.1967)	grad_norm 38.7428 (23.3730)	mem 14369MB
[2022-12-19 20:51:31 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [7/300][50/74]	eta 0:00:21 lr 0.000003	time 0.8707 (0.8856)	loss 209.6334 (239.5029)	grad_norm 25.3660 (23.2457)	mem 14369MB
[2022-12-19 20:51:40 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [7/300][60/74]	eta 0:00:12 lr 0.000003	time 0.8637 (0.8828)	loss 354.9384 (242.3417)	grad_norm 21.4566 (23.8693)	mem 14369MB
[2022-12-19 20:51:49 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [7/300][70/74]	eta 0:00:03 lr 0.000003	time 0.8714 (0.8813)	loss 315.2223 (241.9028)	grad_norm 17.6296 (23.4059)	mem 14369MB
[2022-12-19 20:51:51 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 7 training takes 0:01:05
[2022-12-19 20:51:53 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [8/300][0/74]	eta 0:01:54 lr 0.000003	time 1.5456 (1.5456)	loss 229.3500 (229.3500)	grad_norm 23.2292 (23.2292)	mem 14369MB
[2022-12-19 20:52:02 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [8/300][10/74]	eta 0:00:59 lr 0.000003	time 0.8672 (0.9306)	loss 222.9401 (242.5020)	grad_norm 18.4754 (30.0974)	mem 14369MB
[2022-12-19 20:52:10 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [8/300][20/74]	eta 0:00:48 lr 0.000003	time 0.8691 (0.9053)	loss 220.6843 (238.0975)	grad_norm 19.5207 (25.8712)	mem 14369MB
[2022-12-19 20:52:19 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [8/300][30/74]	eta 0:00:39 lr 0.000003	time 0.8642 (0.8936)	loss 267.4894 (238.5990)	grad_norm 27.3049 (25.2191)	mem 14369MB
[2022-12-19 20:52:28 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [8/300][40/74]	eta 0:00:30 lr 0.000003	time 0.8614 (0.8884)	loss 282.6868 (237.8399)	grad_norm 17.2880 (24.7008)	mem 14369MB
[2022-12-19 20:52:36 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [8/300][50/74]	eta 0:00:21 lr 0.000003	time 0.8671 (0.8850)	loss 336.5230 (243.9200)	grad_norm 21.1084 (24.4383)	mem 14369MB
[2022-12-19 20:52:45 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [8/300][60/74]	eta 0:00:12 lr 0.000003	time 0.8611 (0.8823)	loss 353.1989 (245.6455)	grad_norm 21.3852 (24.1517)	mem 14369MB
[2022-12-19 20:52:54 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [8/300][70/74]	eta 0:00:03 lr 0.000003	time 0.8708 (0.8830)	loss 313.5029 (245.9158)	grad_norm 20.8153 (24.0446)	mem 14369MB
[2022-12-19 20:52:57 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 8 training takes 0:01:05
[2022-12-19 20:52:58 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [9/300][0/74]	eta 0:01:55 lr 0.000004	time 1.5644 (1.5644)	loss 285.0070 (285.0070)	grad_norm 19.8025 (19.8025)	mem 14369MB
[2022-12-19 20:53:07 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [9/300][10/74]	eta 0:00:59 lr 0.000004	time 0.8785 (0.9372)	loss 170.5044 (246.8224)	grad_norm 31.6042 (23.1318)	mem 14369MB
[2022-12-19 20:53:16 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [9/300][20/74]	eta 0:00:48 lr 0.000004	time 0.8796 (0.9064)	loss 195.8303 (242.4223)	grad_norm 27.5042 (24.0230)	mem 14369MB
[2022-12-19 20:53:24 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [9/300][30/74]	eta 0:00:39 lr 0.000004	time 0.8700 (0.8951)	loss 244.7255 (247.3767)	grad_norm 38.1177 (24.7318)	mem 14369MB
[2022-12-19 20:53:33 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [9/300][40/74]	eta 0:00:30 lr 0.000004	time 0.8726 (0.8885)	loss 287.5271 (248.2162)	grad_norm 28.5266 (23.8996)	mem 14369MB
[2022-12-19 20:53:42 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [9/300][50/74]	eta 0:00:21 lr 0.000004	time 0.8817 (0.8855)	loss 205.5521 (245.2838)	grad_norm 20.3044 (23.7901)	mem 14369MB
[2022-12-19 20:53:50 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [9/300][60/74]	eta 0:00:12 lr 0.000004	time 0.8753 (0.8833)	loss 208.1474 (243.3092)	grad_norm 21.0289 (23.7866)	mem 14369MB
[2022-12-19 20:53:59 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [9/300][70/74]	eta 0:00:03 lr 0.000004	time 0.8659 (0.8818)	loss 192.6478 (240.8863)	grad_norm 26.2751 (23.7767)	mem 14369MB
[2022-12-19 20:54:02 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 9 training takes 0:01:05
[2022-12-19 20:54:03 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [10/300][0/74]	eta 0:01:54 lr 0.000004	time 1.5505 (1.5505)	loss 249.4533 (249.4533)	grad_norm 20.7217 (20.7217)	mem 14369MB
[2022-12-19 20:54:12 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [10/300][10/74]	eta 0:00:59 lr 0.000004	time 0.8732 (0.9368)	loss 270.0360 (251.0029)	grad_norm 24.1987 (22.8351)	mem 14369MB
[2022-12-19 20:54:21 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [10/300][20/74]	eta 0:00:48 lr 0.000004	time 0.8796 (0.9062)	loss 217.7529 (242.2253)	grad_norm 15.3427 (23.3344)	mem 14369MB
[2022-12-19 20:54:29 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [10/300][30/74]	eta 0:00:39 lr 0.000004	time 0.8823 (0.8957)	loss 149.2291 (232.5244)	grad_norm 16.2500 (23.2663)	mem 14369MB
[2022-12-19 20:54:38 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [10/300][40/74]	eta 0:00:30 lr 0.000004	time 0.8662 (0.8894)	loss 232.5100 (233.5941)	grad_norm 35.1381 (23.8083)	mem 14369MB
[2022-12-19 20:54:47 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [10/300][50/74]	eta 0:00:21 lr 0.000004	time 0.8782 (0.8866)	loss 229.9519 (233.8901)	grad_norm 27.8124 (24.5821)	mem 14369MB
[2022-12-19 20:54:56 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [10/300][60/74]	eta 0:00:12 lr 0.000004	time 0.8674 (0.8841)	loss 237.5314 (236.1704)	grad_norm 19.2231 (24.3295)	mem 14369MB
[2022-12-19 20:55:04 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [10/300][70/74]	eta 0:00:03 lr 0.000004	time 0.8737 (0.8820)	loss 225.2711 (238.6981)	grad_norm 55.7273 (25.4172)	mem 14369MB
[2022-12-19 20:55:07 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 10 training takes 0:01:05
[2022-12-19 20:55:08 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [11/300][0/74]	eta 0:01:54 lr 0.000004	time 1.5421 (1.5421)	loss 268.8212 (268.8212)	grad_norm 21.8868 (21.8868)	mem 14369MB
[2022-12-19 20:55:17 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [11/300][10/74]	eta 0:00:59 lr 0.000004	time 0.8698 (0.9326)	loss 235.7281 (218.4817)	grad_norm 19.1125 (23.2548)	mem 14369MB
[2022-12-19 20:55:26 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [11/300][20/74]	eta 0:00:48 lr 0.000004	time 0.8877 (0.9032)	loss 185.9660 (227.7276)	grad_norm 32.3428 (25.4778)	mem 14369MB
[2022-12-19 20:55:35 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [11/300][30/74]	eta 0:00:39 lr 0.000004	time 0.8650 (0.8938)	loss 338.9832 (239.2869)	grad_norm 20.8608 (25.0135)	mem 14369MB
[2022-12-19 20:55:43 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [11/300][40/74]	eta 0:00:30 lr 0.000005	time 0.8719 (0.8892)	loss 248.0692 (238.5163)	grad_norm 35.8449 (25.4579)	mem 14369MB
[2022-12-19 20:55:52 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [11/300][50/74]	eta 0:00:21 lr 0.000005	time 0.8909 (0.8863)	loss 205.5041 (236.5308)	grad_norm 38.5992 (26.0391)	mem 14369MB
[2022-12-19 20:56:01 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [11/300][60/74]	eta 0:00:12 lr 0.000005	time 0.8740 (0.8837)	loss 235.5780 (243.0235)	grad_norm 28.6453 (26.2846)	mem 14369MB
[2022-12-19 20:56:09 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [11/300][70/74]	eta 0:00:03 lr 0.000005	time 0.8768 (0.8818)	loss 223.3883 (241.2724)	grad_norm 31.9527 (26.6534)	mem 14369MB
[2022-12-19 20:56:12 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 11 training takes 0:01:05
[2022-12-19 20:56:14 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [12/300][0/74]	eta 0:01:54 lr 0.000005	time 1.5464 (1.5464)	loss 227.7879 (227.7879)	grad_norm 19.3669 (19.3669)	mem 14369MB
[2022-12-19 20:56:22 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [12/300][10/74]	eta 0:00:59 lr 0.000005	time 0.8737 (0.9345)	loss 264.2255 (245.4403)	grad_norm 25.5330 (26.4142)	mem 14369MB
[2022-12-19 20:56:31 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [12/300][20/74]	eta 0:00:48 lr 0.000005	time 0.8678 (0.9061)	loss 318.7979 (235.3556)	grad_norm 32.1683 (26.7329)	mem 14369MB
[2022-12-19 20:56:40 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [12/300][30/74]	eta 0:00:39 lr 0.000005	time 0.8622 (0.8948)	loss 236.6826 (234.7115)	grad_norm 26.9669 (27.2781)	mem 14369MB
[2022-12-19 20:56:48 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [12/300][40/74]	eta 0:00:30 lr 0.000005	time 0.8689 (0.8888)	loss 296.4653 (236.8712)	grad_norm 22.1830 (29.0952)	mem 14369MB
[2022-12-19 20:56:57 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [12/300][50/74]	eta 0:00:21 lr 0.000005	time 0.8705 (0.8862)	loss 195.4223 (235.0915)	grad_norm 47.9287 (29.0971)	mem 14369MB
[2022-12-19 20:57:06 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [12/300][60/74]	eta 0:00:12 lr 0.000005	time 0.8669 (0.8837)	loss 365.2896 (237.5440)	grad_norm 28.7455 (29.6790)	mem 14369MB
[2022-12-19 20:57:15 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [12/300][70/74]	eta 0:00:03 lr 0.000005	time 0.8710 (0.8824)	loss 245.3045 (237.9529)	grad_norm 25.8136 (29.1130)	mem 14369MB
[2022-12-19 20:57:17 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 12 training takes 0:01:05
[2022-12-19 20:57:19 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [13/300][0/74]	eta 0:01:53 lr 0.000005	time 1.5320 (1.5320)	loss 308.7264 (308.7264)	grad_norm 38.0305 (38.0305)	mem 14369MB
[2022-12-19 20:57:27 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [13/300][10/74]	eta 0:00:59 lr 0.000005	time 0.8771 (0.9305)	loss 191.4276 (229.4975)	grad_norm 27.8633 (33.8724)	mem 14369MB
[2022-12-19 20:57:36 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [13/300][20/74]	eta 0:00:48 lr 0.000005	time 0.8868 (0.9007)	loss 184.9854 (241.9411)	grad_norm 20.1724 (34.0084)	mem 14369MB
[2022-12-19 20:57:45 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [13/300][30/74]	eta 0:00:39 lr 0.000005	time 0.8574 (0.8908)	loss 241.1817 (244.9477)	grad_norm 24.0947 (33.8055)	mem 14369MB
[2022-12-19 20:57:54 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [13/300][40/74]	eta 0:00:30 lr 0.000005	time 0.8706 (0.8872)	loss 226.8374 (243.0887)	grad_norm 31.2093 (32.9273)	mem 14369MB
[2022-12-19 20:58:02 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [13/300][50/74]	eta 0:00:21 lr 0.000005	time 0.8858 (0.8851)	loss 193.1376 (238.4820)	grad_norm 31.3607 (32.9119)	mem 14369MB
[2022-12-19 20:58:11 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [13/300][60/74]	eta 0:00:12 lr 0.000005	time 0.8660 (0.8830)	loss 251.4748 (237.3346)	grad_norm 51.1039 (33.8470)	mem 14369MB
[2022-12-19 20:58:20 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [13/300][70/74]	eta 0:00:03 lr 0.000005	time 0.8708 (0.8814)	loss 264.6716 (236.9318)	grad_norm 25.5497 (33.3837)	mem 14369MB
[2022-12-19 20:58:22 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 13 training takes 0:01:05
[2022-12-19 20:58:24 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [14/300][0/74]	eta 0:01:55 lr 0.000005	time 1.5595 (1.5595)	loss 184.0228 (184.0228)	grad_norm 34.8133 (34.8133)	mem 14369MB
[2022-12-19 20:58:33 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [14/300][10/74]	eta 0:00:59 lr 0.000006	time 0.8842 (0.9359)	loss 152.4107 (220.5168)	grad_norm 33.3757 (32.5267)	mem 14369MB
[2022-12-19 20:58:41 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [14/300][20/74]	eta 0:00:48 lr 0.000006	time 0.8760 (0.9058)	loss 242.8943 (224.7540)	grad_norm 37.1888 (37.0862)	mem 14369MB
[2022-12-19 20:58:50 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [14/300][30/74]	eta 0:00:39 lr 0.000006	time 0.8683 (0.8943)	loss 253.8032 (230.2958)	grad_norm 38.0727 (40.8727)	mem 14369MB
[2022-12-19 20:58:59 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [14/300][40/74]	eta 0:00:30 lr 0.000006	time 0.8829 (0.8883)	loss 185.1178 (228.6924)	grad_norm 26.4862 (40.9826)	mem 14369MB
[2022-12-19 20:59:07 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [14/300][50/74]	eta 0:00:21 lr 0.000006	time 0.8685 (0.8846)	loss 208.1751 (230.3363)	grad_norm 45.6921 (41.1335)	mem 14369MB
[2022-12-19 20:59:16 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [14/300][60/74]	eta 0:00:12 lr 0.000006	time 0.8762 (0.8821)	loss 188.1945 (232.0018)	grad_norm 73.4596 (43.4019)	mem 14369MB
[2022-12-19 20:59:25 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [14/300][70/74]	eta 0:00:03 lr 0.000006	time 0.8725 (0.8809)	loss 250.5475 (232.5905)	grad_norm 37.4681 (43.4070)	mem 14369MB
[2022-12-19 20:59:27 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 14 training takes 0:01:05
[2022-12-19 20:59:29 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [15/300][0/74]	eta 0:01:54 lr 0.000006	time 1.5429 (1.5429)	loss 217.7287 (217.7287)	grad_norm 57.6048 (57.6048)	mem 14369MB
[2022-12-19 20:59:38 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [15/300][10/74]	eta 0:00:59 lr 0.000006	time 0.8713 (0.9324)	loss 226.4281 (221.0995)	grad_norm 90.9497 (63.0273)	mem 14369MB
[2022-12-19 20:59:46 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [15/300][20/74]	eta 0:00:48 lr 0.000006	time 0.8859 (0.9028)	loss 201.6974 (227.2596)	grad_norm 44.4809 (56.4738)	mem 14369MB
[2022-12-19 20:59:55 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [15/300][30/74]	eta 0:00:39 lr 0.000006	time 0.8675 (0.8920)	loss 246.7165 (230.8075)	grad_norm 40.4685 (55.5087)	mem 14369MB
[2022-12-19 21:00:04 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [15/300][40/74]	eta 0:00:30 lr 0.000006	time 0.8731 (0.8885)	loss 243.7334 (227.9633)	grad_norm 53.8414 (55.8122)	mem 14369MB
[2022-12-19 21:00:13 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [15/300][50/74]	eta 0:00:21 lr 0.000006	time 0.8868 (0.8855)	loss 188.7482 (230.1606)	grad_norm 37.7151 (56.3236)	mem 14369MB
[2022-12-19 21:00:21 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [15/300][60/74]	eta 0:00:12 lr 0.000006	time 0.8684 (0.8833)	loss 257.5141 (231.5582)	grad_norm 102.3498 (61.9265)	mem 14369MB
[2022-12-19 21:00:30 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [15/300][70/74]	eta 0:00:03 lr 0.000006	time 0.8669 (0.8817)	loss 264.6739 (230.4049)	grad_norm 64.6951 (62.3400)	mem 14369MB
[2022-12-19 21:00:33 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 15 training takes 0:01:05
[2022-12-19 21:00:34 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [16/300][0/74]	eta 0:01:53 lr 0.000006	time 1.5353 (1.5353)	loss 255.8934 (255.8934)	grad_norm 96.9780 (96.9780)	mem 14369MB
[2022-12-19 21:00:43 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [16/300][10/74]	eta 0:00:59 lr 0.000006	time 0.8644 (0.9315)	loss 231.8626 (233.5409)	grad_norm 163.9852 (90.8253)	mem 14369MB
[2022-12-19 21:00:52 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [16/300][20/74]	eta 0:00:48 lr 0.000006	time 0.8733 (0.9041)	loss 163.4695 (227.6631)	grad_norm 64.8111 (96.0598)	mem 14369MB
[2022-12-19 21:01:00 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [16/300][30/74]	eta 0:00:39 lr 0.000006	time 0.8679 (0.8939)	loss 222.2928 (226.3289)	grad_norm 208.9660 (103.1491)	mem 14369MB
[2022-12-19 21:01:09 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [16/300][40/74]	eta 0:00:30 lr 0.000006	time 0.8713 (0.8884)	loss 218.7441 (224.9350)	grad_norm 143.0504 (108.9497)	mem 14369MB
[2022-12-19 21:01:18 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [16/300][50/74]	eta 0:00:21 lr 0.000007	time 0.8770 (0.8851)	loss 153.9115 (224.2124)	grad_norm 190.9071 (115.2751)	mem 14369MB
[2022-12-19 21:01:26 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [16/300][60/74]	eta 0:00:12 lr 0.000007	time 0.8916 (0.8838)	loss 180.8853 (222.3237)	grad_norm 230.9767 (132.9954)	mem 14369MB
[2022-12-19 21:01:35 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [16/300][70/74]	eta 0:00:03 lr 0.000007	time 0.8789 (0.8823)	loss 161.8091 (221.7755)	grad_norm 241.6733 (142.7463)	mem 14369MB
[2022-12-19 21:01:38 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 16 training takes 0:01:05
[2022-12-19 21:01:39 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [17/300][0/74]	eta 0:01:54 lr 0.000007	time 1.5451 (1.5451)	loss 206.2572 (206.2572)	grad_norm 226.5376 (226.5376)	mem 14369MB
[2022-12-19 21:01:48 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [17/300][10/74]	eta 0:00:59 lr 0.000007	time 0.8803 (0.9333)	loss 148.9986 (203.9939)	grad_norm 534.9740 (281.3478)	mem 14369MB
[2022-12-19 21:01:57 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [17/300][20/74]	eta 0:00:48 lr 0.000007	time 0.8683 (0.9044)	loss 208.6870 (201.8146)	grad_norm 414.7499 (344.9503)	mem 14369MB
[2022-12-19 21:02:06 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [17/300][30/74]	eta 0:00:39 lr 0.000007	time 0.8704 (0.8948)	loss 152.9690 (193.2514)	grad_norm 1196.2616 (463.8576)	mem 14369MB
[2022-12-19 21:02:14 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [17/300][40/74]	eta 0:00:30 lr 0.000007	time 0.8874 (0.8892)	loss 154.3786 (194.0892)	grad_norm 679.2129 (638.8117)	mem 14369MB
[2022-12-19 21:02:23 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [17/300][50/74]	eta 0:00:21 lr 0.000007	time 0.8691 (0.8855)	loss 161.0736 (189.8613)	grad_norm 2759.4643 (1086.4830)	mem 14369MB
[2022-12-19 21:02:32 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [17/300][60/74]	eta 0:00:12 lr 0.000007	time 0.8649 (0.8827)	loss 172.3895 (181.4916)	grad_norm 6731.4420 (1807.6780)	mem 14369MB
[2022-12-19 21:02:40 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [17/300][70/74]	eta 0:00:03 lr 0.000007	time 0.8776 (0.8812)	loss 113.9731 (171.9090)	grad_norm 7247.1310 (2114.8781)	mem 14369MB
[2022-12-19 21:02:43 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 17 training takes 0:01:05
[2022-12-19 21:02:44 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [18/300][0/74]	eta 0:01:54 lr 0.000007	time 1.5423 (1.5423)	loss 72.2535 (72.2535)	grad_norm 3838.2876 (3838.2876)	mem 14369MB
[2022-12-19 21:02:53 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [18/300][10/74]	eta 0:00:59 lr 0.000007	time 0.8688 (0.9343)	loss 41.1455 (60.1311)	grad_norm 2726.6247 (3549.4115)	mem 14369MB
[2022-12-19 21:03:02 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [18/300][20/74]	eta 0:00:48 lr 0.000007	time 0.8688 (0.9061)	loss 94.3122 (59.2571)	grad_norm 2534.1853 (3575.5334)	mem 14369MB
[2022-12-19 21:03:11 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [18/300][30/74]	eta 0:00:39 lr 0.000007	time 0.8703 (0.8962)	loss 72.4679 (59.9865)	grad_norm 4999.6650 (3711.3752)	mem 14369MB
[2022-12-19 21:03:19 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [18/300][40/74]	eta 0:00:30 lr 0.000007	time 0.8733 (0.8909)	loss 23.5355 (54.4319)	grad_norm 4148.7894 (3819.3557)	mem 14369MB
[2022-12-19 21:03:28 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [18/300][50/74]	eta 0:00:21 lr 0.000007	time 0.8726 (0.8878)	loss 73.4319 (53.5012)	grad_norm 1700.8415 (3677.2359)	mem 14369MB
[2022-12-19 21:03:37 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [18/300][60/74]	eta 0:00:12 lr 0.000007	time 0.8707 (0.8860)	loss 29.7028 (51.1212)	grad_norm 3505.4367 (3538.7583)	mem 14369MB
[2022-12-19 21:03:46 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [18/300][70/74]	eta 0:00:03 lr 0.000007	time 0.8689 (0.8844)	loss 13.8794 (51.4528)	grad_norm 2656.4636 (3388.3012)	mem 14369MB
[2022-12-19 21:03:48 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 18 training takes 0:01:05
[2022-12-19 21:03:50 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [19/300][0/74]	eta 0:01:55 lr 0.000007	time 1.5561 (1.5561)	loss 20.4433 (20.4433)	grad_norm 3202.7451 (3202.7451)	mem 14369MB
[2022-12-19 21:03:58 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [19/300][10/74]	eta 0:00:59 lr 0.000007	time 0.8842 (0.9321)	loss 52.1817 (35.3357)	grad_norm 3560.8441 (3297.5880)	mem 14369MB
[2022-12-19 21:04:07 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [19/300][20/74]	eta 0:00:48 lr 0.000008	time 0.8812 (0.9052)	loss 40.2760 (45.6352)	grad_norm 2014.2913 (2842.9550)	mem 14369MB
[2022-12-19 21:04:16 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [19/300][30/74]	eta 0:00:39 lr 0.000008	time 0.8813 (0.8963)	loss 8.0256 (39.9396)	grad_norm 952.6083 (2475.1826)	mem 14369MB
[2022-12-19 21:04:25 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [19/300][40/74]	eta 0:00:30 lr 0.000008	time 0.8756 (0.8931)	loss 66.8860 (47.9769)	grad_norm 1870.2042 (2289.7728)	mem 14369MB
[2022-12-19 21:04:34 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [19/300][50/74]	eta 0:00:21 lr 0.000008	time 0.8775 (0.8903)	loss 15.4956 (46.7016)	grad_norm 1515.5625 (2246.5878)	mem 14369MB
[2022-12-19 21:04:42 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [19/300][60/74]	eta 0:00:12 lr 0.000008	time 0.8882 (0.8886)	loss 11.3955 (46.2528)	grad_norm 837.5250 (2097.6647)	mem 14369MB
[2022-12-19 21:04:51 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [19/300][70/74]	eta 0:00:03 lr 0.000008	time 0.8899 (0.8871)	loss 14.0907 (45.0318)	grad_norm 1124.8062 (2060.2749)	mem 14369MB
[2022-12-19 21:04:54 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 19 training takes 0:01:05
[2022-12-19 21:04:55 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [20/300][0/74]	eta 0:01:55 lr 0.000008	time 1.5593 (1.5593)	loss 15.1863 (15.1863)	grad_norm 1334.7594 (1334.7594)	mem 14369MB
[2022-12-19 21:05:04 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [20/300][10/74]	eta 0:00:59 lr 0.000008	time 0.8715 (0.9348)	loss 73.1152 (52.9330)	grad_norm 2072.0858 (1498.7440)	mem 14369MB
[2022-12-19 21:05:13 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [20/300][20/74]	eta 0:00:48 lr 0.000008	time 0.8804 (0.9058)	loss 102.4821 (56.3049)	grad_norm 1801.3193 (1656.2908)	mem 14369MB
[2022-12-19 21:05:22 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [20/300][30/74]	eta 0:00:39 lr 0.000008	time 0.8775 (0.8948)	loss 55.4800 (52.8843)	grad_norm 1692.5446 (1627.5352)	mem 14369MB
[2022-12-19 21:05:30 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [20/300][40/74]	eta 0:00:30 lr 0.000008	time 0.8746 (0.8911)	loss 85.6245 (49.1652)	grad_norm 1164.9215 (1526.8451)	mem 14369MB
[2022-12-19 21:05:39 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [20/300][50/74]	eta 0:00:21 lr 0.000008	time 0.8697 (0.8883)	loss 98.0187 (45.5219)	grad_norm 2767.5495 (1588.4821)	mem 14369MB
[2022-12-19 21:05:48 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [20/300][60/74]	eta 0:00:12 lr 0.000008	time 0.8817 (0.8866)	loss 10.1477 (44.3617)	grad_norm 1052.8850 (1598.5100)	mem 14369MB
[2022-12-19 21:05:57 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [20/300][70/74]	eta 0:00:03 lr 0.000008	time 0.8848 (0.8855)	loss 53.5296 (44.2241)	grad_norm 406.1157 (1577.0558)	mem 14369MB
[2022-12-19 21:05:59 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 20 training takes 0:01:05
[2022-12-19 21:05:59 RepVGGplus-tinyism] (helpers.py 207): INFO ./output/repvggplus/RepVGGplus-tinyism/default/ckpt_epoch_20.pth saving......
[2022-12-19 21:06:00 RepVGGplus-tinyism] (helpers.py 209): INFO ./output/repvggplus/RepVGGplus-tinyism/default/ckpt_epoch_20.pth saved !!!
[2022-12-19 21:06:02 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [21/300][0/74]	eta 0:01:53 lr 0.000008	time 1.5289 (1.5289)	loss 12.0549 (12.0549)	grad_norm 1047.6591 (1047.6591)	mem 14369MB
[2022-12-19 21:06:11 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [21/300][10/74]	eta 0:00:59 lr 0.000008	time 0.8687 (0.9289)	loss 79.3831 (61.8447)	grad_norm 1185.1194 (1283.4479)	mem 14369MB
[2022-12-19 21:06:19 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [21/300][20/74]	eta 0:00:48 lr 0.000008	time 0.8693 (0.9001)	loss 52.7107 (55.1395)	grad_norm 996.5014 (1228.9460)	mem 14369MB
[2022-12-19 21:06:28 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [21/300][30/74]	eta 0:00:39 lr 0.000008	time 0.8630 (0.8899)	loss 77.8969 (51.4639)	grad_norm 592.3963 (1139.3189)	mem 14369MB
[2022-12-19 21:06:37 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [21/300][40/74]	eta 0:00:30 lr 0.000008	time 0.8601 (0.8839)	loss 20.2824 (48.2009)	grad_norm 2612.2960 (1186.7488)	mem 14369MB
[2022-12-19 21:06:45 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [21/300][50/74]	eta 0:00:21 lr 0.000008	time 0.8656 (0.8816)	loss 15.1494 (42.9505)	grad_norm 1378.4905 (1169.7579)	mem 14369MB
[2022-12-19 21:06:54 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [21/300][60/74]	eta 0:00:12 lr 0.000008	time 0.8690 (0.8789)	loss 15.3095 (42.8964)	grad_norm 1037.4708 (1211.9278)	mem 14369MB
[2022-12-19 21:07:03 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [21/300][70/74]	eta 0:00:03 lr 0.000008	time 0.8721 (0.8781)	loss 58.1000 (43.4148)	grad_norm 726.9696 (1168.0775)	mem 14369MB
[2022-12-19 21:07:05 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 21 training takes 0:01:04
[2022-12-19 21:07:07 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [22/300][0/74]	eta 0:01:51 lr 0.000008	time 1.5011 (1.5011)	loss 10.4385 (10.4385)	grad_norm 932.7221 (932.7221)	mem 14369MB
[2022-12-19 21:07:16 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [22/300][10/74]	eta 0:00:59 lr 0.000008	time 0.8691 (0.9242)	loss 39.1292 (32.6976)	grad_norm 997.7418 (925.5805)	mem 14369MB
[2022-12-19 21:07:24 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [22/300][20/74]	eta 0:00:48 lr 0.000008	time 0.8636 (0.8990)	loss 80.7620 (43.9126)	grad_norm 550.9508 (936.5376)	mem 14369MB
[2022-12-19 21:07:33 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [22/300][30/74]	eta 0:00:39 lr 0.000008	time 0.8636 (0.8939)	loss 35.6780 (40.5570)	grad_norm 844.6272 (974.0396)	mem 14369MB
[2022-12-19 21:07:42 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [22/300][40/74]	eta 0:00:30 lr 0.000008	time 0.8726 (0.8875)	loss 94.0568 (44.4740)	grad_norm 707.3214 (969.9578)	mem 14369MB
[2022-12-19 21:07:50 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [22/300][50/74]	eta 0:00:21 lr 0.000008	time 0.8613 (0.8830)	loss 41.5735 (44.9738)	grad_norm 1644.4705 (1057.7174)	mem 14369MB
[2022-12-19 21:07:59 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [22/300][60/74]	eta 0:00:12 lr 0.000008	time 0.8563 (0.8800)	loss 53.9677 (44.2263)	grad_norm 2721.1507 (1086.0131)	mem 14369MB
[2022-12-19 21:08:08 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [22/300][70/74]	eta 0:00:03 lr 0.000008	time 0.8579 (0.8781)	loss 82.1447 (43.2563)	grad_norm 1302.5601 (1097.1566)	mem 14369MB
[2022-12-19 21:08:10 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 22 training takes 0:01:04
[2022-12-19 21:08:12 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [23/300][0/74]	eta 0:01:51 lr 0.000008	time 1.5122 (1.5122)	loss 54.3440 (54.3440)	grad_norm 1624.7887 (1624.7887)	mem 14369MB
[2022-12-19 21:08:20 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [23/300][10/74]	eta 0:00:59 lr 0.000008	time 0.8625 (0.9283)	loss 12.4174 (33.0777)	grad_norm 1129.3733 (1119.0364)	mem 14369MB
[2022-12-19 21:08:29 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [23/300][20/74]	eta 0:00:48 lr 0.000008	time 0.8779 (0.8999)	loss 36.1184 (44.3950)	grad_norm 835.4692 (1025.3533)	mem 14369MB
[2022-12-19 21:08:38 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [23/300][30/74]	eta 0:00:39 lr 0.000008	time 0.8768 (0.8892)	loss 23.1152 (49.6716)	grad_norm 2040.2345 (1068.6787)	mem 14369MB
[2022-12-19 21:08:46 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [23/300][40/74]	eta 0:00:30 lr 0.000008	time 0.8637 (0.8837)	loss 12.0381 (47.8173)	grad_norm 938.5390 (1089.1441)	mem 14369MB
[2022-12-19 21:08:55 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [23/300][50/74]	eta 0:00:21 lr 0.000008	time 0.8596 (0.8796)	loss 8.2028 (44.3078)	grad_norm 690.8111 (1099.5990)	mem 14369MB
[2022-12-19 21:09:04 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [23/300][60/74]	eta 0:00:12 lr 0.000008	time 0.8606 (0.8776)	loss 43.8842 (44.7183)	grad_norm 770.4969 (1106.0774)	mem 14369MB
[2022-12-19 21:09:12 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [23/300][70/74]	eta 0:00:03 lr 0.000008	time 0.8681 (0.8762)	loss 72.3745 (43.8688)	grad_norm 1352.7707 (1116.4498)	mem 14369MB
[2022-12-19 21:09:15 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 23 training takes 0:01:04
[2022-12-19 21:09:17 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [24/300][0/74]	eta 0:01:52 lr 0.000008	time 1.5142 (1.5142)	loss 83.5504 (83.5504)	grad_norm 892.3394 (892.3394)	mem 14369MB
[2022-12-19 21:09:25 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [24/300][10/74]	eta 0:00:59 lr 0.000008	time 0.8575 (0.9219)	loss 7.2102 (28.9018)	grad_norm 762.3869 (1345.3596)	mem 14369MB
[2022-12-19 21:09:34 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [24/300][20/74]	eta 0:00:48 lr 0.000008	time 0.8682 (0.8945)	loss 12.2234 (34.4734)	grad_norm 1392.5802 (1276.8557)	mem 14369MB
[2022-12-19 21:09:42 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [24/300][30/74]	eta 0:00:38 lr 0.000008	time 0.8754 (0.8855)	loss 11.5426 (36.4941)	grad_norm 1095.3894 (1270.0526)	mem 14369MB
[2022-12-19 21:09:51 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [24/300][40/74]	eta 0:00:30 lr 0.000008	time 0.8658 (0.8827)	loss 70.2604 (42.1700)	grad_norm 1793.8661 (1185.0942)	mem 14369MB
[2022-12-19 21:10:00 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [24/300][50/74]	eta 0:00:21 lr 0.000008	time 0.8706 (0.8806)	loss 14.9018 (44.4619)	grad_norm 1408.4962 (1205.5921)	mem 14369MB
[2022-12-19 21:10:09 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [24/300][60/74]	eta 0:00:12 lr 0.000008	time 0.8773 (0.8797)	loss 55.3793 (43.7583)	grad_norm 932.2645 (1253.4705)	mem 14369MB
[2022-12-19 21:10:17 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [24/300][70/74]	eta 0:00:03 lr 0.000008	time 0.8606 (0.8780)	loss 54.1246 (42.7903)	grad_norm 840.6119 (1273.0995)	mem 14369MB
[2022-12-19 21:10:20 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 24 training takes 0:01:04
[2022-12-19 21:10:21 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [25/300][0/74]	eta 0:01:51 lr 0.000008	time 1.5025 (1.5025)	loss 11.3830 (11.3830)	grad_norm 1032.8554 (1032.8554)	mem 14369MB
[2022-12-19 21:10:30 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [25/300][10/74]	eta 0:00:59 lr 0.000008	time 0.8809 (0.9268)	loss 20.7747 (30.2767)	grad_norm 1043.5407 (980.6631)	mem 14369MB
[2022-12-19 21:10:39 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [25/300][20/74]	eta 0:00:48 lr 0.000008	time 0.8569 (0.8984)	loss 18.1440 (40.8464)	grad_norm 1661.1972 (965.0370)	mem 14369MB
[2022-12-19 21:10:47 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [25/300][30/74]	eta 0:00:39 lr 0.000008	time 0.8657 (0.8873)	loss 15.3584 (40.5662)	grad_norm 1078.4932 (1053.5656)	mem 14369MB
[2022-12-19 21:10:56 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [25/300][40/74]	eta 0:00:30 lr 0.000008	time 1.0698 (0.8859)	loss 9.7139 (38.2658)	grad_norm 636.1702 (1064.2349)	mem 14369MB
[2022-12-19 21:11:05 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [25/300][50/74]	eta 0:00:21 lr 0.000008	time 0.8821 (0.8821)	loss 55.8042 (43.2125)	grad_norm 1319.2054 (1060.6513)	mem 14369MB
[2022-12-19 21:11:14 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [25/300][60/74]	eta 0:00:12 lr 0.000008	time 0.8575 (0.8793)	loss 13.7623 (40.5794)	grad_norm 1900.1411 (1082.3164)	mem 14369MB
[2022-12-19 21:11:22 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [25/300][70/74]	eta 0:00:03 lr 0.000008	time 0.8686 (0.8775)	loss 51.2621 (41.8466)	grad_norm 1311.3502 (1079.9517)	mem 14369MB
[2022-12-19 21:11:25 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 25 training takes 0:01:04
[2022-12-19 21:11:26 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [26/300][0/74]	eta 0:01:52 lr 0.000008	time 1.5143 (1.5143)	loss 60.8483 (60.8483)	grad_norm 710.5454 (710.5454)	mem 14369MB
[2022-12-19 21:11:35 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [26/300][10/74]	eta 0:00:59 lr 0.000008	time 0.8624 (0.9258)	loss 15.3812 (41.5747)	grad_norm 1426.2416 (1167.6422)	mem 14369MB
[2022-12-19 21:11:44 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [26/300][20/74]	eta 0:00:48 lr 0.000008	time 0.8718 (0.8986)	loss 107.0516 (42.4741)	grad_norm 451.0016 (1020.0919)	mem 14369MB
[2022-12-19 21:11:52 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [26/300][30/74]	eta 0:00:39 lr 0.000008	time 0.8679 (0.8884)	loss 11.7782 (44.8587)	grad_norm 817.3532 (933.9176)	mem 14369MB
[2022-12-19 21:12:01 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [26/300][40/74]	eta 0:00:30 lr 0.000008	time 0.8782 (0.8836)	loss 32.4093 (40.4802)	grad_norm 405.6339 (994.1481)	mem 14369MB
[2022-12-19 21:12:10 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [26/300][50/74]	eta 0:00:21 lr 0.000008	time 0.8600 (0.8804)	loss 68.1599 (39.4123)	grad_norm 1415.6429 (1001.9651)	mem 14369MB
[2022-12-19 21:12:18 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [26/300][60/74]	eta 0:00:12 lr 0.000008	time 0.8685 (0.8779)	loss 41.8599 (39.2471)	grad_norm 1099.9324 (1017.3422)	mem 14369MB
[2022-12-19 21:12:27 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [26/300][70/74]	eta 0:00:03 lr 0.000008	time 0.8648 (0.8764)	loss 89.8777 (41.8062)	grad_norm 917.1606 (1031.6085)	mem 14369MB
[2022-12-19 21:12:29 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 26 training takes 0:01:04
[2022-12-19 21:12:31 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [27/300][0/74]	eta 0:01:53 lr 0.000008	time 1.5325 (1.5325)	loss 44.7717 (44.7717)	grad_norm 448.1975 (448.1975)	mem 14369MB
[2022-12-19 21:12:40 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [27/300][10/74]	eta 0:00:59 lr 0.000008	time 0.8730 (0.9287)	loss 46.1232 (41.2367)	grad_norm 705.1038 (805.1926)	mem 14369MB
[2022-12-19 21:12:48 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [27/300][20/74]	eta 0:00:48 lr 0.000008	time 0.8614 (0.9006)	loss 86.8367 (44.8095)	grad_norm 569.6580 (798.6086)	mem 14369MB
[2022-12-19 21:12:57 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [27/300][30/74]	eta 0:00:39 lr 0.000008	time 0.8635 (0.8893)	loss 130.3235 (42.4949)	grad_norm 1050.5016 (857.8983)	mem 14369MB
[2022-12-19 21:13:06 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [27/300][40/74]	eta 0:00:30 lr 0.000008	time 0.8621 (0.8833)	loss 15.6611 (38.1514)	grad_norm 1208.2645 (855.1113)	mem 14369MB
[2022-12-19 21:13:14 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [27/300][50/74]	eta 0:00:21 lr 0.000008	time 0.8642 (0.8797)	loss 47.1294 (38.3723)	grad_norm 1151.4331 (884.2133)	mem 14369MB
[2022-12-19 21:13:23 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [27/300][60/74]	eta 0:00:12 lr 0.000008	time 0.8679 (0.8787)	loss 14.9235 (37.4183)	grad_norm 1238.3161 (882.1030)	mem 14369MB
[2022-12-19 21:13:32 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [27/300][70/74]	eta 0:00:03 lr 0.000008	time 0.8647 (0.8767)	loss 73.2997 (39.9888)	grad_norm 981.5434 (897.0393)	mem 14369MB
[2022-12-19 21:13:34 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 27 training takes 0:01:04
[2022-12-19 21:13:36 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [28/300][0/74]	eta 0:01:53 lr 0.000008	time 1.5298 (1.5298)	loss 29.6154 (29.6154)	grad_norm 448.2390 (448.2390)	mem 14369MB
[2022-12-19 21:13:44 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [28/300][10/74]	eta 0:00:59 lr 0.000008	time 0.8595 (0.9265)	loss 20.0785 (34.5760)	grad_norm 1636.8278 (887.5694)	mem 14369MB
[2022-12-19 21:13:53 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [28/300][20/74]	eta 0:00:48 lr 0.000008	time 0.8661 (0.8984)	loss 20.5190 (40.5241)	grad_norm 1396.0984 (873.4086)	mem 14369MB
[2022-12-19 21:14:02 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [28/300][30/74]	eta 0:00:39 lr 0.000008	time 0.8537 (0.8879)	loss 34.6648 (41.0649)	grad_norm 3608.6960 (938.2846)	mem 14369MB
[2022-12-19 21:14:10 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [28/300][40/74]	eta 0:00:30 lr 0.000008	time 0.8814 (0.8828)	loss 10.0462 (44.4684)	grad_norm 733.6753 (916.7037)	mem 14369MB
[2022-12-19 21:14:19 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [28/300][50/74]	eta 0:00:21 lr 0.000008	time 0.8647 (0.8790)	loss 12.2942 (42.5425)	grad_norm 1188.4837 (922.7079)	mem 14369MB
[2022-12-19 21:14:28 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [28/300][60/74]	eta 0:00:12 lr 0.000008	time 0.8618 (0.8765)	loss 15.4572 (41.1513)	grad_norm 1009.9193 (950.7617)	mem 14369MB
[2022-12-19 21:14:36 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [28/300][70/74]	eta 0:00:03 lr 0.000008	time 0.8786 (0.8749)	loss 17.6149 (40.8007)	grad_norm 510.7502 (935.6538)	mem 14369MB
[2022-12-19 21:14:39 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 28 training takes 0:01:04
[2022-12-19 21:14:40 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [29/300][0/74]	eta 0:01:52 lr 0.000008	time 1.5159 (1.5159)	loss 7.4227 (7.4227)	grad_norm 519.6333 (519.6333)	mem 14369MB
[2022-12-19 21:14:49 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [29/300][10/74]	eta 0:00:59 lr 0.000008	time 0.8586 (0.9267)	loss 15.8190 (45.2619)	grad_norm 1524.6685 (755.9216)	mem 14369MB
[2022-12-19 21:14:58 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [29/300][20/74]	eta 0:00:48 lr 0.000008	time 0.8782 (0.8973)	loss 8.0659 (37.5578)	grad_norm 453.9628 (805.4939)	mem 14369MB
[2022-12-19 21:15:06 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [29/300][30/74]	eta 0:00:39 lr 0.000008	time 0.8757 (0.8880)	loss 42.8658 (34.5240)	grad_norm 477.2388 (814.8394)	mem 14369MB
[2022-12-19 21:15:15 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [29/300][40/74]	eta 0:00:30 lr 0.000008	time 0.8787 (0.8832)	loss 13.7628 (40.7163)	grad_norm 1224.2092 (835.7254)	mem 14369MB
[2022-12-19 21:15:24 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [29/300][50/74]	eta 0:00:21 lr 0.000008	time 0.8592 (0.8801)	loss 11.5510 (41.3425)	grad_norm 1069.5557 (854.3761)	mem 14369MB
[2022-12-19 21:15:32 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [29/300][60/74]	eta 0:00:12 lr 0.000008	time 0.8842 (0.8785)	loss 16.6825 (39.9715)	grad_norm 445.2463 (847.2260)	mem 14369MB
[2022-12-19 21:15:41 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [29/300][70/74]	eta 0:00:03 lr 0.000008	time 0.8674 (0.8769)	loss 30.8683 (39.6332)	grad_norm 891.3805 (847.4256)	mem 14369MB
[2022-12-19 21:15:44 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 29 training takes 0:01:04
[2022-12-19 21:15:45 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [30/300][0/74]	eta 0:01:53 lr 0.000008	time 1.5326 (1.5326)	loss 20.1339 (20.1339)	grad_norm 430.0028 (430.0028)	mem 14369MB
[2022-12-19 21:15:54 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [30/300][10/74]	eta 0:00:59 lr 0.000008	time 0.8599 (0.9326)	loss 10.6958 (40.9256)	grad_norm 1002.0326 (822.3476)	mem 14369MB
[2022-12-19 21:16:03 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [30/300][20/74]	eta 0:00:48 lr 0.000008	time 0.8684 (0.9015)	loss 32.6174 (43.0128)	grad_norm 1099.2640 (854.9603)	mem 14369MB
[2022-12-19 21:16:11 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [30/300][30/74]	eta 0:00:39 lr 0.000008	time 0.8724 (0.8919)	loss 19.0176 (39.8642)	grad_norm 522.1361 (782.8320)	mem 14369MB
[2022-12-19 21:16:20 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [30/300][40/74]	eta 0:00:30 lr 0.000008	time 0.8695 (0.8862)	loss 10.9719 (36.7354)	grad_norm 1018.8379 (824.0644)	mem 14369MB
[2022-12-19 21:16:29 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [30/300][50/74]	eta 0:00:21 lr 0.000008	time 0.8888 (0.8833)	loss 27.6423 (39.4033)	grad_norm 548.7601 (844.9204)	mem 14369MB
[2022-12-19 21:16:37 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [30/300][60/74]	eta 0:00:12 lr 0.000008	time 0.8751 (0.8816)	loss 58.8093 (38.6108)	grad_norm 2003.7099 (850.4885)	mem 14369MB
[2022-12-19 21:16:46 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [30/300][70/74]	eta 0:00:03 lr 0.000008	time 0.8644 (0.8807)	loss 9.4657 (38.9336)	grad_norm 796.9150 (840.4084)	mem 14369MB
[2022-12-19 21:16:49 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 30 training takes 0:01:05
[2022-12-19 21:16:50 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [31/300][0/74]	eta 0:01:51 lr 0.000008	time 1.5029 (1.5029)	loss 49.0944 (49.0944)	grad_norm 702.2953 (702.2953)	mem 14369MB
[2022-12-19 21:16:59 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [31/300][10/74]	eta 0:00:59 lr 0.000008	time 0.8586 (0.9259)	loss 6.8920 (42.9323)	grad_norm 732.4382 (751.7560)	mem 14369MB
[2022-12-19 21:17:08 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [31/300][20/74]	eta 0:00:48 lr 0.000008	time 0.8764 (0.8989)	loss 36.8367 (39.6646)	grad_norm 286.0143 (780.6197)	mem 14369MB
[2022-12-19 21:17:16 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [31/300][30/74]	eta 0:00:39 lr 0.000008	time 0.8584 (0.8873)	loss 9.5386 (34.4983)	grad_norm 1190.2653 (842.1988)	mem 14369MB
[2022-12-19 21:17:25 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [31/300][40/74]	eta 0:00:30 lr 0.000008	time 0.8656 (0.8832)	loss 19.6737 (39.7087)	grad_norm 1606.8438 (827.2342)	mem 14369MB
[2022-12-19 21:17:34 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [31/300][50/74]	eta 0:00:21 lr 0.000008	time 0.8689 (0.8807)	loss 65.3168 (41.4116)	grad_norm 458.9540 (810.3992)	mem 14369MB
[2022-12-19 21:17:42 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [31/300][60/74]	eta 0:00:12 lr 0.000008	time 0.8649 (0.8785)	loss 47.1473 (39.0355)	grad_norm 471.7632 (819.3394)	mem 14369MB
[2022-12-19 21:17:51 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [31/300][70/74]	eta 0:00:03 lr 0.000008	time 0.8854 (0.8777)	loss 24.4025 (37.9666)	grad_norm 742.2988 (819.8114)	mem 14369MB
[2022-12-19 21:17:54 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 31 training takes 0:01:04
[2022-12-19 21:17:55 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [32/300][0/74]	eta 0:01:53 lr 0.000008	time 1.5374 (1.5374)	loss 16.4481 (16.4481)	grad_norm 487.4591 (487.4591)	mem 14369MB
[2022-12-19 21:18:04 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [32/300][10/74]	eta 0:00:59 lr 0.000008	time 0.8619 (0.9315)	loss 10.5405 (30.1685)	grad_norm 1310.8751 (712.5106)	mem 14369MB
[2022-12-19 21:18:13 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [32/300][20/74]	eta 0:00:48 lr 0.000008	time 0.8684 (0.9024)	loss 19.3303 (32.4986)	grad_norm 889.0851 (654.5682)	mem 14369MB
[2022-12-19 21:18:21 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [32/300][30/74]	eta 0:00:39 lr 0.000008	time 0.8709 (0.8923)	loss 7.5066 (32.2286)	grad_norm 616.3498 (672.4540)	mem 14369MB
[2022-12-19 21:18:30 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [32/300][40/74]	eta 0:00:30 lr 0.000008	time 0.8761 (0.8872)	loss 53.4277 (33.7387)	grad_norm 609.6186 (704.0793)	mem 14369MB
[2022-12-19 21:18:39 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [32/300][50/74]	eta 0:00:21 lr 0.000008	time 0.8686 (0.8846)	loss 101.8796 (36.6024)	grad_norm 324.9299 (697.8609)	mem 14369MB
[2022-12-19 21:18:47 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [32/300][60/74]	eta 0:00:12 lr 0.000008	time 0.8736 (0.8821)	loss 12.4191 (36.1745)	grad_norm 1146.2977 (705.3737)	mem 14369MB
[2022-12-19 21:18:56 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [32/300][70/74]	eta 0:00:03 lr 0.000008	time 0.8626 (0.8804)	loss 8.7801 (34.8026)	grad_norm 1474.7742 (792.5245)	mem 14369MB
[2022-12-19 21:18:59 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 32 training takes 0:01:05
[2022-12-19 21:19:00 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [33/300][0/74]	eta 0:01:51 lr 0.000008	time 1.5112 (1.5112)	loss 10.4558 (10.4558)	grad_norm 989.9498 (989.9498)	mem 14369MB
[2022-12-19 21:19:09 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [33/300][10/74]	eta 0:00:59 lr 0.000008	time 0.8750 (0.9282)	loss 70.5672 (24.4978)	grad_norm 511.5831 (979.5225)	mem 14369MB
[2022-12-19 21:19:18 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [33/300][20/74]	eta 0:00:48 lr 0.000008	time 0.8762 (0.9023)	loss 50.2916 (30.1096)	grad_norm 201.0230 (940.9786)	mem 14369MB
[2022-12-19 21:19:26 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [33/300][30/74]	eta 0:00:39 lr 0.000008	time 0.8803 (0.8914)	loss 17.7256 (31.7405)	grad_norm 590.6277 (850.5443)	mem 14369MB
[2022-12-19 21:19:35 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [33/300][40/74]	eta 0:00:30 lr 0.000008	time 0.8686 (0.8868)	loss 12.9370 (31.9590)	grad_norm 1537.9175 (846.3818)	mem 14369MB
[2022-12-19 21:19:44 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [33/300][50/74]	eta 0:00:21 lr 0.000008	time 0.8654 (0.8837)	loss 21.2831 (30.2573)	grad_norm 1618.6869 (845.5557)	mem 14369MB
[2022-12-19 21:19:52 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [33/300][60/74]	eta 0:00:12 lr 0.000008	time 0.8693 (0.8812)	loss 42.8799 (30.6304)	grad_norm 403.0343 (826.3844)	mem 14369MB
[2022-12-19 21:20:01 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [33/300][70/74]	eta 0:00:03 lr 0.000008	time 0.8633 (0.8800)	loss 9.4883 (31.2923)	grad_norm 1198.4727 (820.2141)	mem 14369MB
[2022-12-19 21:20:04 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 33 training takes 0:01:04
[2022-12-19 21:20:05 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [34/300][0/74]	eta 0:01:51 lr 0.000008	time 1.5112 (1.5112)	loss 44.6552 (44.6552)	grad_norm 852.4885 (852.4885)	mem 14369MB
[2022-12-19 21:20:14 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [34/300][10/74]	eta 0:00:59 lr 0.000008	time 0.8613 (0.9251)	loss 9.6433 (35.4144)	grad_norm 821.6389 (737.3787)	mem 14369MB
[2022-12-19 21:20:23 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [34/300][20/74]	eta 0:00:48 lr 0.000008	time 0.8657 (0.8971)	loss 25.0763 (34.6585)	grad_norm 2146.9730 (771.8268)	mem 14369MB
[2022-12-19 21:20:31 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [34/300][30/74]	eta 0:00:39 lr 0.000008	time 0.8603 (0.8871)	loss 38.5585 (31.1228)	grad_norm 802.7320 (791.6141)	mem 14369MB
[2022-12-19 21:20:40 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [34/300][40/74]	eta 0:00:29 lr 0.000008	time 0.8589 (0.8818)	loss 16.0133 (29.6759)	grad_norm 2103.9343 (868.6048)	mem 14369MB
[2022-12-19 21:20:49 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [34/300][50/74]	eta 0:00:21 lr 0.000008	time 0.8607 (0.8792)	loss 84.3283 (30.2749)	grad_norm 292.7672 (850.2709)	mem 14369MB
[2022-12-19 21:20:57 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [34/300][60/74]	eta 0:00:12 lr 0.000008	time 0.8796 (0.8773)	loss 27.2824 (28.1347)	grad_norm 808.2298 (858.9266)	mem 14369MB
[2022-12-19 21:21:06 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [34/300][70/74]	eta 0:00:03 lr 0.000008	time 0.8639 (0.8770)	loss 38.5611 (28.3332)	grad_norm 1566.8841 (832.0257)	mem 14369MB
[2022-12-19 21:21:08 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 34 training takes 0:01:04
[2022-12-19 21:21:10 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [35/300][0/74]	eta 0:01:52 lr 0.000008	time 1.5258 (1.5258)	loss 12.9573 (12.9573)	grad_norm 817.4587 (817.4587)	mem 14369MB
[2022-12-19 21:21:19 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [35/300][10/74]	eta 0:00:59 lr 0.000008	time 0.8659 (0.9257)	loss 30.8118 (16.9983)	grad_norm 795.2308 (752.6007)	mem 14369MB
[2022-12-19 21:21:27 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [35/300][20/74]	eta 0:00:48 lr 0.000008	time 0.8821 (0.8998)	loss 7.4920 (20.1718)	grad_norm 529.0127 (774.7536)	mem 14369MB
[2022-12-19 21:21:36 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [35/300][30/74]	eta 0:00:39 lr 0.000008	time 0.8572 (0.8880)	loss 35.6794 (24.0204)	grad_norm 1541.0390 (837.8393)	mem 14369MB
[2022-12-19 21:21:45 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [35/300][40/74]	eta 0:00:30 lr 0.000008	time 0.8686 (0.8831)	loss 39.7824 (23.6767)	grad_norm 728.9072 (836.4198)	mem 14369MB
[2022-12-19 21:21:53 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [35/300][50/74]	eta 0:00:21 lr 0.000008	time 0.8654 (0.8798)	loss 8.1014 (22.5501)	grad_norm 991.5358 (850.2307)	mem 14369MB
[2022-12-19 21:22:02 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [35/300][60/74]	eta 0:00:12 lr 0.000008	time 0.8655 (0.8778)	loss 30.7005 (21.0060)	grad_norm 245.4682 (820.2863)	mem 14369MB
[2022-12-19 21:22:11 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [35/300][70/74]	eta 0:00:03 lr 0.000008	time 0.8728 (0.8763)	loss 8.2678 (20.8408)	grad_norm 446.0423 (790.5773)	mem 14369MB
[2022-12-19 21:22:13 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 35 training takes 0:01:04
[2022-12-19 21:22:15 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [36/300][0/74]	eta 0:01:52 lr 0.000008	time 1.5138 (1.5138)	loss 6.2664 (6.2664)	grad_norm 573.7101 (573.7101)	mem 14369MB
[2022-12-19 21:22:23 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [36/300][10/74]	eta 0:00:59 lr 0.000008	time 0.8683 (0.9311)	loss 12.6166 (18.1161)	grad_norm 691.9271 (684.4473)	mem 14369MB
[2022-12-19 21:22:32 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [36/300][20/74]	eta 0:00:48 lr 0.000008	time 0.8571 (0.8993)	loss 8.5646 (16.1399)	grad_norm 1300.9525 (808.2197)	mem 14369MB
[2022-12-19 21:22:41 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [36/300][30/74]	eta 0:00:39 lr 0.000008	time 0.8615 (0.8877)	loss 8.0605 (16.1650)	grad_norm 883.4977 (814.7581)	mem 14369MB
[2022-12-19 21:22:49 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [36/300][40/74]	eta 0:00:29 lr 0.000008	time 0.8737 (0.8819)	loss 20.6044 (17.1495)	grad_norm 490.0303 (868.6417)	mem 14369MB
[2022-12-19 21:22:58 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [36/300][50/74]	eta 0:00:21 lr 0.000008	time 0.8704 (0.8795)	loss 8.2373 (17.8446)	grad_norm 785.4303 (890.6361)	mem 14369MB
[2022-12-19 21:23:07 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [36/300][60/74]	eta 0:00:12 lr 0.000008	time 0.8761 (0.8777)	loss 16.2806 (17.0204)	grad_norm 908.8764 (914.3228)	mem 14369MB
[2022-12-19 21:23:15 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [36/300][70/74]	eta 0:00:03 lr 0.000008	time 0.8708 (0.8766)	loss 7.2422 (16.4988)	grad_norm 484.4365 (918.8647)	mem 14369MB
[2022-12-19 21:23:18 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 36 training takes 0:01:04
[2022-12-19 21:23:19 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [37/300][0/74]	eta 0:01:52 lr 0.000008	time 1.5181 (1.5181)	loss 20.8512 (20.8512)	grad_norm 987.7949 (987.7949)	mem 14369MB
[2022-12-19 21:23:28 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [37/300][10/74]	eta 0:00:59 lr 0.000008	time 0.8827 (0.9301)	loss 13.2834 (11.9334)	grad_norm 294.9992 (824.1078)	mem 14369MB
[2022-12-19 21:23:37 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [37/300][20/74]	eta 0:00:48 lr 0.000008	time 0.8654 (0.9021)	loss 8.0024 (12.8820)	grad_norm 963.9309 (918.8224)	mem 14369MB
[2022-12-19 21:23:46 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [37/300][30/74]	eta 0:00:39 lr 0.000008	time 0.8887 (0.8933)	loss 14.8667 (12.8881)	grad_norm 968.8393 (937.5878)	mem 14369MB
[2022-12-19 21:23:54 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [37/300][40/74]	eta 0:00:30 lr 0.000008	time 0.8748 (0.8869)	loss 16.3157 (12.9908)	grad_norm 534.0280 (931.6139)	mem 14369MB
[2022-12-19 21:24:03 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [37/300][50/74]	eta 0:00:21 lr 0.000008	time 0.8654 (0.8833)	loss 10.6065 (12.8566)	grad_norm 1001.9542 (944.6210)	mem 14369MB
[2022-12-19 21:24:12 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [37/300][60/74]	eta 0:00:12 lr 0.000008	time 0.8728 (0.8816)	loss 11.7309 (12.7375)	grad_norm 422.1401 (909.5746)	mem 14369MB
[2022-12-19 21:24:20 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [37/300][70/74]	eta 0:00:03 lr 0.000008	time 0.8739 (0.8795)	loss 10.6271 (13.0946)	grad_norm 599.8855 (921.6776)	mem 14369MB
[2022-12-19 21:24:23 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 37 training takes 0:01:04
[2022-12-19 21:24:24 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [38/300][0/74]	eta 0:01:52 lr 0.000008	time 1.5156 (1.5156)	loss 8.6678 (8.6678)	grad_norm 376.5947 (376.5947)	mem 14369MB
[2022-12-19 21:24:33 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [38/300][10/74]	eta 0:00:59 lr 0.000008	time 0.8736 (0.9240)	loss 7.9070 (12.8045)	grad_norm 396.6692 (904.5697)	mem 14369MB
[2022-12-19 21:24:42 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [38/300][20/74]	eta 0:00:48 lr 0.000008	time 0.8674 (0.8980)	loss 18.7025 (14.3881)	grad_norm 948.5913 (1078.1519)	mem 14369MB
[2022-12-19 21:24:51 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [38/300][30/74]	eta 0:00:39 lr 0.000008	time 0.8695 (0.8906)	loss 20.4372 (13.7013)	grad_norm 574.1338 (1055.5879)	mem 14369MB
[2022-12-19 21:24:59 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [38/300][40/74]	eta 0:00:30 lr 0.000007	time 0.8672 (0.8862)	loss 9.7851 (12.7618)	grad_norm 695.7745 (980.9427)	mem 14369MB
[2022-12-19 21:25:08 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [38/300][50/74]	eta 0:00:21 lr 0.000007	time 0.8696 (0.8824)	loss 6.0048 (12.5217)	grad_norm 438.1742 (961.3698)	mem 14369MB
[2022-12-19 21:25:17 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [38/300][60/74]	eta 0:00:12 lr 0.000007	time 0.8702 (0.8801)	loss 11.3039 (12.3120)	grad_norm 816.7064 (934.9600)	mem 14369MB
[2022-12-19 21:25:25 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [38/300][70/74]	eta 0:00:03 lr 0.000007	time 0.8635 (0.8785)	loss 11.9694 (12.2068)	grad_norm 1122.0451 (939.2316)	mem 14369MB
[2022-12-19 21:25:28 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 38 training takes 0:01:04
[2022-12-19 21:25:29 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [39/300][0/74]	eta 0:01:52 lr 0.000007	time 1.5171 (1.5171)	loss 18.4770 (18.4770)	grad_norm 1859.0866 (1859.0866)	mem 14369MB
[2022-12-19 21:25:38 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [39/300][10/74]	eta 0:00:59 lr 0.000007	time 0.8689 (0.9286)	loss 7.2036 (12.5741)	grad_norm 564.5692 (989.8397)	mem 14369MB
[2022-12-19 21:25:47 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [39/300][20/74]	eta 0:00:48 lr 0.000007	time 0.8689 (0.8993)	loss 12.4684 (12.0220)	grad_norm 487.6755 (931.4912)	mem 14369MB
[2022-12-19 21:25:55 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [39/300][30/74]	eta 0:00:39 lr 0.000007	time 0.8826 (0.8898)	loss 10.3886 (11.5839)	grad_norm 783.2351 (920.5824)	mem 14369MB
[2022-12-19 21:26:04 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [39/300][40/74]	eta 0:00:30 lr 0.000007	time 0.8712 (0.8839)	loss 11.2006 (11.4984)	grad_norm 749.4106 (950.5038)	mem 14369MB
[2022-12-19 21:26:13 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [39/300][50/74]	eta 0:00:21 lr 0.000007	time 0.8612 (0.8801)	loss 9.4482 (11.7202)	grad_norm 619.6728 (937.4443)	mem 14369MB
[2022-12-19 21:26:21 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [39/300][60/74]	eta 0:00:12 lr 0.000007	time 0.8737 (0.8780)	loss 7.5283 (11.5669)	grad_norm 733.5339 (925.9676)	mem 14369MB
[2022-12-19 21:26:30 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [39/300][70/74]	eta 0:00:03 lr 0.000007	time 0.8679 (0.8763)	loss 6.0892 (11.6200)	grad_norm 387.1294 (921.6906)	mem 14369MB
[2022-12-19 21:26:33 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 39 training takes 0:01:04
[2022-12-19 21:26:34 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [40/300][0/74]	eta 0:01:52 lr 0.000007	time 1.5149 (1.5149)	loss 11.0502 (11.0502)	grad_norm 616.7947 (616.7947)	mem 14369MB
[2022-12-19 21:26:43 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [40/300][10/74]	eta 0:00:59 lr 0.000007	time 0.8616 (0.9262)	loss 20.2529 (12.3783)	grad_norm 1307.7047 (927.1898)	mem 14369MB
[2022-12-19 21:26:51 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [40/300][20/74]	eta 0:00:48 lr 0.000007	time 0.8697 (0.8965)	loss 8.6055 (12.4033)	grad_norm 422.1825 (971.7249)	mem 14369MB
[2022-12-19 21:27:00 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [40/300][30/74]	eta 0:00:39 lr 0.000007	time 0.8769 (0.8890)	loss 11.9223 (11.5804)	grad_norm 514.2252 (894.6590)	mem 14369MB
[2022-12-19 21:27:09 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [40/300][40/74]	eta 0:00:30 lr 0.000007	time 0.8736 (0.8855)	loss 9.9627 (11.6343)	grad_norm 575.4241 (887.0976)	mem 14369MB
[2022-12-19 21:27:18 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [40/300][50/74]	eta 0:00:21 lr 0.000007	time 0.8702 (0.8828)	loss 9.3263 (11.8427)	grad_norm 645.5465 (941.4697)	mem 14369MB
[2022-12-19 21:27:26 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [40/300][60/74]	eta 0:00:12 lr 0.000007	time 0.8632 (0.8802)	loss 8.4152 (11.7110)	grad_norm 713.6034 (959.2684)	mem 14369MB
[2022-12-19 21:27:35 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [40/300][70/74]	eta 0:00:03 lr 0.000007	time 0.8618 (0.8783)	loss 14.7178 (11.7028)	grad_norm 1416.0996 (990.4188)	mem 14369MB
[2022-12-19 21:27:37 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 40 training takes 0:01:04
[2022-12-19 21:27:37 RepVGGplus-tinyism] (helpers.py 207): INFO ./output/repvggplus/RepVGGplus-tinyism/default/ckpt_epoch_40.pth saving......
[2022-12-19 21:27:39 RepVGGplus-tinyism] (helpers.py 209): INFO ./output/repvggplus/RepVGGplus-tinyism/default/ckpt_epoch_40.pth saved !!!
[2022-12-19 21:27:40 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [41/300][0/74]	eta 0:01:51 lr 0.000007	time 1.5004 (1.5004)	loss 8.4000 (8.4000)	grad_norm 575.9747 (575.9747)	mem 14369MB
[2022-12-19 21:27:49 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [41/300][10/74]	eta 0:00:58 lr 0.000007	time 0.8676 (0.9218)	loss 11.4019 (10.6471)	grad_norm 1700.8308 (1017.4402)	mem 14369MB
[2022-12-19 21:27:57 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [41/300][20/74]	eta 0:00:48 lr 0.000007	time 0.8708 (0.8960)	loss 6.9901 (11.2668)	grad_norm 248.1964 (981.7934)	mem 14369MB
[2022-12-19 21:28:06 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [41/300][30/74]	eta 0:00:38 lr 0.000007	time 0.8616 (0.8856)	loss 9.3099 (11.2128)	grad_norm 1062.2584 (965.1148)	mem 14369MB
[2022-12-19 21:28:15 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [41/300][40/74]	eta 0:00:29 lr 0.000007	time 0.8630 (0.8814)	loss 8.7885 (10.3613)	grad_norm 573.3259 (873.9507)	mem 14369MB
[2022-12-19 21:28:23 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [41/300][50/74]	eta 0:00:21 lr 0.000007	time 0.8724 (0.8782)	loss 15.6940 (10.3227)	grad_norm 614.4015 (846.8125)	mem 14369MB
[2022-12-19 21:28:32 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [41/300][60/74]	eta 0:00:12 lr 0.000007	time 0.8769 (0.8764)	loss 8.9805 (10.6090)	grad_norm 645.0660 (846.5540)	mem 14369MB
[2022-12-19 21:28:41 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [41/300][70/74]	eta 0:00:03 lr 0.000007	time 0.8743 (0.8755)	loss 12.7619 (10.6959)	grad_norm 865.6099 (832.1156)	mem 14369MB
[2022-12-19 21:28:43 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 41 training takes 0:01:04
[2022-12-19 21:28:45 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [42/300][0/74]	eta 0:01:51 lr 0.000007	time 1.5007 (1.5007)	loss 8.1958 (8.1958)	grad_norm 1108.4165 (1108.4165)	mem 14369MB
[2022-12-19 21:28:53 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [42/300][10/74]	eta 0:00:59 lr 0.000007	time 0.8661 (0.9234)	loss 14.5691 (10.7372)	grad_norm 1294.7710 (907.1477)	mem 14369MB
[2022-12-19 21:29:02 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [42/300][20/74]	eta 0:00:48 lr 0.000007	time 0.8601 (0.8956)	loss 10.2282 (10.4417)	grad_norm 1278.2478 (839.8357)	mem 14369MB
[2022-12-19 21:29:11 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [42/300][30/74]	eta 0:00:39 lr 0.000007	time 0.8763 (0.8872)	loss 9.0175 (9.8031)	grad_norm 753.0743 (799.3450)	mem 14369MB
[2022-12-19 21:29:19 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [42/300][40/74]	eta 0:00:30 lr 0.000007	time 0.8677 (0.8824)	loss 4.7430 (9.8100)	grad_norm 331.9289 (774.8479)	mem 14369MB
[2022-12-19 21:29:28 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [42/300][50/74]	eta 0:00:21 lr 0.000007	time 0.8759 (0.8796)	loss 11.3803 (9.8857)	grad_norm 607.9583 (763.1589)	mem 14369MB
[2022-12-19 21:29:37 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [42/300][60/74]	eta 0:00:12 lr 0.000007	time 0.8594 (0.8779)	loss 14.7674 (10.2035)	grad_norm 1359.6250 (814.7792)	mem 14369MB
[2022-12-19 21:29:46 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [42/300][70/74]	eta 0:00:03 lr 0.000007	time 0.8615 (0.8788)	loss 13.7946 (10.1826)	grad_norm 910.9403 (821.8154)	mem 14369MB
[2022-12-19 21:29:48 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 42 training takes 0:01:04
[2022-12-19 21:29:50 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [43/300][0/74]	eta 0:01:52 lr 0.000007	time 1.5212 (1.5212)	loss 5.1035 (5.1035)	grad_norm 640.3018 (640.3018)	mem 14369MB
[2022-12-19 21:29:58 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [43/300][10/74]	eta 0:00:59 lr 0.000007	time 0.8639 (0.9281)	loss 8.2866 (7.3479)	grad_norm 779.6147 (568.2219)	mem 14369MB
[2022-12-19 21:30:07 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [43/300][20/74]	eta 0:00:48 lr 0.000007	time 0.8581 (0.8983)	loss 9.8635 (8.5468)	grad_norm 1071.3298 (661.4200)	mem 14369MB
[2022-12-19 21:30:16 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [43/300][30/74]	eta 0:00:39 lr 0.000007	time 0.8621 (0.8868)	loss 12.5342 (8.5562)	grad_norm 603.9058 (649.2585)	mem 14369MB
[2022-12-19 21:30:24 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [43/300][40/74]	eta 0:00:29 lr 0.000007	time 0.8597 (0.8818)	loss 15.3155 (9.4296)	grad_norm 896.1071 (662.3662)	mem 14369MB
[2022-12-19 21:30:33 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [43/300][50/74]	eta 0:00:21 lr 0.000007	time 0.8626 (0.8782)	loss 10.7597 (9.5381)	grad_norm 882.5605 (705.3901)	mem 14369MB
[2022-12-19 21:30:42 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [43/300][60/74]	eta 0:00:12 lr 0.000007	time 0.8652 (0.8763)	loss 14.0441 (9.5329)	grad_norm 576.7985 (721.2384)	mem 14369MB
[2022-12-19 21:30:50 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [43/300][70/74]	eta 0:00:03 lr 0.000007	time 0.8934 (0.8755)	loss 9.0062 (9.3372)	grad_norm 607.7777 (700.8391)	mem 14369MB
[2022-12-19 21:30:53 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 43 training takes 0:01:04
[2022-12-19 21:30:54 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [44/300][0/74]	eta 0:01:51 lr 0.000007	time 1.5087 (1.5087)	loss 4.8621 (4.8621)	grad_norm 313.2374 (313.2374)	mem 14369MB
[2022-12-19 21:31:03 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [44/300][10/74]	eta 0:00:59 lr 0.000007	time 0.8851 (0.9247)	loss 15.6389 (11.4078)	grad_norm 1071.8702 (872.4487)	mem 14369MB
[2022-12-19 21:31:12 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [44/300][20/74]	eta 0:00:48 lr 0.000007	time 0.8632 (0.8967)	loss 10.9327 (11.4355)	grad_norm 895.8214 (849.0225)	mem 14369MB
[2022-12-19 21:31:20 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [44/300][30/74]	eta 0:00:39 lr 0.000007	time 0.8811 (0.8865)	loss 9.5527 (11.1150)	grad_norm 919.9043 (811.4212)	mem 14369MB
[2022-12-19 21:31:29 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [44/300][40/74]	eta 0:00:29 lr 0.000007	time 0.8623 (0.8809)	loss 12.2848 (10.8671)	grad_norm 1072.3805 (807.0453)	mem 14369MB
[2022-12-19 21:31:38 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [44/300][50/74]	eta 0:00:21 lr 0.000007	time 0.8616 (0.8776)	loss 9.9137 (10.7534)	grad_norm 1300.6816 (826.1541)	mem 14369MB
[2022-12-19 21:31:46 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [44/300][60/74]	eta 0:00:12 lr 0.000007	time 0.8765 (0.8755)	loss 8.3632 (10.5706)	grad_norm 432.9709 (837.4209)	mem 14369MB
[2022-12-19 21:31:55 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [44/300][70/74]	eta 0:00:03 lr 0.000007	time 0.8654 (0.8744)	loss 11.3614 (10.2279)	grad_norm 1276.7179 (808.9340)	mem 14369MB
[2022-12-19 21:31:57 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 44 training takes 0:01:04
[2022-12-19 21:31:59 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [45/300][0/74]	eta 0:01:51 lr 0.000007	time 1.5058 (1.5058)	loss 13.2022 (13.2022)	grad_norm 1011.3662 (1011.3662)	mem 14369MB
[2022-12-19 21:32:08 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [45/300][10/74]	eta 0:00:59 lr 0.000007	time 0.8698 (0.9256)	loss 6.9571 (9.4249)	grad_norm 492.7673 (743.6930)	mem 14369MB
[2022-12-19 21:32:16 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [45/300][20/74]	eta 0:00:48 lr 0.000007	time 0.8616 (0.8968)	loss 5.9851 (9.0428)	grad_norm 811.0194 (743.0148)	mem 14369MB
[2022-12-19 21:32:25 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [45/300][30/74]	eta 0:00:39 lr 0.000007	time 0.8642 (0.8870)	loss 9.5449 (9.3324)	grad_norm 365.4440 (726.6139)	mem 14369MB
[2022-12-19 21:32:34 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [45/300][40/74]	eta 0:00:29 lr 0.000007	time 0.8570 (0.8809)	loss 10.0829 (9.3540)	grad_norm 901.3865 (752.6458)	mem 14369MB
[2022-12-19 21:32:42 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [45/300][50/74]	eta 0:00:21 lr 0.000007	time 0.8717 (0.8779)	loss 10.7762 (9.5532)	grad_norm 748.5282 (743.9507)	mem 14369MB
[2022-12-19 21:32:51 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [45/300][60/74]	eta 0:00:12 lr 0.000007	time 0.8670 (0.8760)	loss 11.0690 (9.5462)	grad_norm 1060.1521 (748.6030)	mem 14369MB
[2022-12-19 21:33:00 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [45/300][70/74]	eta 0:00:03 lr 0.000007	time 0.8635 (0.8749)	loss 12.2384 (9.5140)	grad_norm 2152.5457 (750.2371)	mem 14369MB
[2022-12-19 21:33:02 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 45 training takes 0:01:04
[2022-12-19 21:33:04 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [46/300][0/74]	eta 0:01:52 lr 0.000007	time 1.5156 (1.5156)	loss 12.6783 (12.6783)	grad_norm 517.0070 (517.0070)	mem 14369MB
[2022-12-19 21:33:12 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [46/300][10/74]	eta 0:00:59 lr 0.000007	time 0.8589 (0.9241)	loss 9.5447 (9.4826)	grad_norm 992.5659 (659.7844)	mem 14369MB
[2022-12-19 21:33:21 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [46/300][20/74]	eta 0:00:48 lr 0.000007	time 0.8721 (0.8975)	loss 10.8176 (9.5466)	grad_norm 628.6366 (656.8606)	mem 14369MB
[2022-12-19 21:33:30 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [46/300][30/74]	eta 0:00:39 lr 0.000007	time 0.8754 (0.8868)	loss 4.7887 (9.0603)	grad_norm 364.3784 (647.9238)	mem 14369MB
[2022-12-19 21:33:38 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [46/300][40/74]	eta 0:00:29 lr 0.000007	time 0.8577 (0.8820)	loss 4.6865 (8.7874)	grad_norm 621.3959 (608.7639)	mem 14369MB
[2022-12-19 21:33:47 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [46/300][50/74]	eta 0:00:21 lr 0.000007	time 0.8672 (0.8788)	loss 15.2063 (8.9248)	grad_norm 984.8291 (645.0285)	mem 14369MB
[2022-12-19 21:33:56 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [46/300][60/74]	eta 0:00:12 lr 0.000007	time 0.8809 (0.8777)	loss 6.7460 (9.1276)	grad_norm 489.1517 (660.6457)	mem 14369MB
[2022-12-19 21:34:04 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [46/300][70/74]	eta 0:00:03 lr 0.000007	time 0.8828 (0.8766)	loss 6.5418 (9.0093)	grad_norm 450.0490 (648.6239)	mem 14369MB
[2022-12-19 21:34:07 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 46 training takes 0:01:04
[2022-12-19 21:34:08 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [47/300][0/74]	eta 0:01:51 lr 0.000007	time 1.5044 (1.5044)	loss 5.1100 (5.1100)	grad_norm 699.7689 (699.7689)	mem 14369MB
[2022-12-19 21:34:17 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [47/300][10/74]	eta 0:00:59 lr 0.000007	time 0.8740 (0.9285)	loss 8.7584 (8.1726)	grad_norm 481.5093 (557.0896)	mem 14369MB
[2022-12-19 21:34:26 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [47/300][20/74]	eta 0:00:48 lr 0.000007	time 0.8634 (0.8988)	loss 5.9898 (7.2222)	grad_norm 667.7420 (535.5754)	mem 14369MB
[2022-12-19 21:34:35 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [47/300][30/74]	eta 0:00:39 lr 0.000007	time 0.8820 (0.8887)	loss 14.5548 (7.9186)	grad_norm 511.2594 (604.2864)	mem 14369MB
[2022-12-19 21:34:43 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [47/300][40/74]	eta 0:00:30 lr 0.000007	time 0.8609 (0.8845)	loss 13.6262 (8.9652)	grad_norm 1298.6560 (683.0755)	mem 14369MB
[2022-12-19 21:34:52 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [47/300][50/74]	eta 0:00:21 lr 0.000007	time 0.8647 (0.8809)	loss 7.3042 (8.8878)	grad_norm 706.8080 (697.8245)	mem 14369MB
[2022-12-19 21:35:01 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [47/300][60/74]	eta 0:00:12 lr 0.000007	time 0.8759 (0.8797)	loss 9.4576 (9.1273)	grad_norm 668.3599 (699.3197)	mem 14369MB
[2022-12-19 21:35:09 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [47/300][70/74]	eta 0:00:03 lr 0.000007	time 0.8601 (0.8781)	loss 10.3118 (9.0371)	grad_norm 1226.3475 (710.4966)	mem 14369MB
[2022-12-19 21:35:12 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 47 training takes 0:01:04
[2022-12-19 21:35:13 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [48/300][0/74]	eta 0:01:52 lr 0.000007	time 1.5233 (1.5233)	loss 8.5126 (8.5126)	grad_norm 628.7867 (628.7867)	mem 14369MB
[2022-12-19 21:35:22 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [48/300][10/74]	eta 0:00:59 lr 0.000007	time 0.8668 (0.9267)	loss 7.5536 (10.6603)	grad_norm 740.5352 (911.3215)	mem 14369MB
[2022-12-19 21:35:31 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [48/300][20/74]	eta 0:00:48 lr 0.000007	time 0.8762 (0.8985)	loss 6.8503 (10.7153)	grad_norm 471.9289 (889.3376)	mem 14369MB
[2022-12-19 21:35:39 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [48/300][30/74]	eta 0:00:39 lr 0.000007	time 0.8791 (0.8889)	loss 7.9850 (10.0039)	grad_norm 561.0427 (841.3048)	mem 14369MB
[2022-12-19 21:35:48 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [48/300][40/74]	eta 0:00:30 lr 0.000007	time 0.8820 (0.8841)	loss 4.5312 (9.4163)	grad_norm 366.4813 (787.3470)	mem 14369MB
[2022-12-19 21:35:57 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [48/300][50/74]	eta 0:00:21 lr 0.000007	time 0.8784 (0.8822)	loss 14.0395 (9.5613)	grad_norm 341.5261 (739.4549)	mem 14369MB
[2022-12-19 21:36:06 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [48/300][60/74]	eta 0:00:12 lr 0.000007	time 0.8679 (0.8806)	loss 8.4558 (9.7176)	grad_norm 551.7583 (739.0125)	mem 14369MB
[2022-12-19 21:36:14 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [48/300][70/74]	eta 0:00:03 lr 0.000007	time 0.8807 (0.8798)	loss 9.0809 (9.6524)	grad_norm 580.5989 (718.5879)	mem 14369MB
[2022-12-19 21:36:17 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 48 training takes 0:01:04
[2022-12-19 21:36:18 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [49/300][0/74]	eta 0:01:52 lr 0.000007	time 1.5173 (1.5173)	loss 9.7227 (9.7227)	grad_norm 851.5070 (851.5070)	mem 14369MB
[2022-12-19 21:36:27 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [49/300][10/74]	eta 0:00:59 lr 0.000007	time 0.8727 (0.9315)	loss 7.2224 (7.1368)	grad_norm 497.4747 (507.4913)	mem 14369MB
[2022-12-19 21:36:36 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [49/300][20/74]	eta 0:00:48 lr 0.000007	time 0.8804 (0.9013)	loss 13.0521 (8.0895)	grad_norm 377.0459 (595.6001)	mem 14369MB
[2022-12-19 21:36:44 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [49/300][30/74]	eta 0:00:39 lr 0.000007	time 0.8718 (0.8898)	loss 6.7412 (7.9695)	grad_norm 444.5055 (590.1216)	mem 14369MB
[2022-12-19 21:36:53 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [49/300][40/74]	eta 0:00:30 lr 0.000007	time 0.8667 (0.8859)	loss 8.5230 (8.0397)	grad_norm 710.6989 (576.6009)	mem 14369MB
[2022-12-19 21:37:02 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [49/300][50/74]	eta 0:00:21 lr 0.000007	time 0.8813 (0.8828)	loss 9.1211 (8.3106)	grad_norm 552.9674 (620.6937)	mem 14369MB
[2022-12-19 21:37:11 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [49/300][60/74]	eta 0:00:12 lr 0.000007	time 0.8640 (0.8806)	loss 9.5363 (8.2288)	grad_norm 987.5363 (612.6404)	mem 14369MB
[2022-12-19 21:37:19 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [49/300][70/74]	eta 0:00:03 lr 0.000007	time 0.8736 (0.8793)	loss 5.4267 (8.2877)	grad_norm 343.7482 (613.8419)	mem 14369MB
[2022-12-19 21:37:22 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 49 training takes 0:01:04
[2022-12-19 21:37:23 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [50/300][0/74]	eta 0:01:52 lr 0.000007	time 1.5174 (1.5174)	loss 9.5304 (9.5304)	grad_norm 571.0782 (571.0782)	mem 14369MB
[2022-12-19 21:37:32 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [50/300][10/74]	eta 0:00:59 lr 0.000007	time 0.8624 (0.9237)	loss 16.3226 (12.1253)	grad_norm 768.6225 (763.3254)	mem 14369MB
[2022-12-19 21:37:41 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [50/300][20/74]	eta 0:00:48 lr 0.000007	time 0.8825 (0.8984)	loss 7.2400 (9.7656)	grad_norm 474.8703 (663.2225)	mem 14369MB
[2022-12-19 21:37:49 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [50/300][30/74]	eta 0:00:39 lr 0.000007	time 0.8631 (0.8868)	loss 6.5331 (9.0183)	grad_norm 489.9251 (638.3994)	mem 14369MB
[2022-12-19 21:37:58 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [50/300][40/74]	eta 0:00:30 lr 0.000007	time 0.8726 (0.8826)	loss 9.9955 (9.2481)	grad_norm 435.9868 (616.6980)	mem 14369MB
[2022-12-19 21:38:07 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [50/300][50/74]	eta 0:00:21 lr 0.000007	time 0.8608 (0.8786)	loss 7.1897 (9.2783)	grad_norm 539.8201 (628.4275)	mem 14369MB
[2022-12-19 21:38:15 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [50/300][60/74]	eta 0:00:12 lr 0.000007	time 0.8724 (0.8766)	loss 6.1861 (8.7669)	grad_norm 401.1898 (614.7245)	mem 14369MB
[2022-12-19 21:38:24 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [50/300][70/74]	eta 0:00:03 lr 0.000007	time 0.8597 (0.8750)	loss 4.5946 (8.6142)	grad_norm 589.8101 (612.1353)	mem 14369MB
[2022-12-19 21:38:26 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 50 training takes 0:01:04
[2022-12-19 21:38:28 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [51/300][0/74]	eta 0:01:51 lr 0.000007	time 1.5074 (1.5074)	loss 11.5966 (11.5966)	grad_norm 591.6546 (591.6546)	mem 14369MB
[2022-12-19 21:38:37 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [51/300][10/74]	eta 0:00:59 lr 0.000007	time 0.8628 (0.9234)	loss 9.4887 (7.5575)	grad_norm 1045.7039 (592.3614)	mem 14369MB
[2022-12-19 21:38:45 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [51/300][20/74]	eta 0:00:48 lr 0.000007	time 0.8740 (0.8966)	loss 6.8690 (7.9716)	grad_norm 530.0824 (603.6648)	mem 14369MB
[2022-12-19 21:38:54 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [51/300][30/74]	eta 0:00:38 lr 0.000007	time 0.8572 (0.8860)	loss 7.6494 (8.3154)	grad_norm 886.8422 (625.8574)	mem 14369MB
[2022-12-19 21:39:03 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [51/300][40/74]	eta 0:00:29 lr 0.000007	time 0.8607 (0.8819)	loss 8.3206 (8.1353)	grad_norm 1021.9587 (586.9185)	mem 14369MB
[2022-12-19 21:39:11 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [51/300][50/74]	eta 0:00:21 lr 0.000007	time 0.8663 (0.8786)	loss 12.7789 (8.1303)	grad_norm 330.6723 (597.7101)	mem 14369MB
[2022-12-19 21:39:20 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [51/300][60/74]	eta 0:00:12 lr 0.000007	time 0.8656 (0.8769)	loss 10.3163 (8.3005)	grad_norm 586.0380 (603.7156)	mem 14369MB
[2022-12-19 21:39:29 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [51/300][70/74]	eta 0:00:03 lr 0.000007	time 0.8638 (0.8757)	loss 6.9818 (8.4254)	grad_norm 782.7183 (614.5197)	mem 14369MB
[2022-12-19 21:39:31 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 51 training takes 0:01:04
[2022-12-19 21:39:33 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [52/300][0/74]	eta 0:01:51 lr 0.000007	time 1.5037 (1.5037)	loss 6.3706 (6.3706)	grad_norm 503.1243 (503.1243)	mem 14369MB
[2022-12-19 21:39:41 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [52/300][10/74]	eta 0:00:58 lr 0.000007	time 0.8620 (0.9214)	loss 7.2406 (8.6915)	grad_norm 561.4142 (570.6822)	mem 14369MB
[2022-12-19 21:39:50 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [52/300][20/74]	eta 0:00:48 lr 0.000007	time 0.8741 (0.8958)	loss 5.1710 (8.5316)	grad_norm 196.9758 (574.2636)	mem 14369MB
[2022-12-19 21:39:59 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [52/300][30/74]	eta 0:00:39 lr 0.000007	time 0.8641 (0.8878)	loss 8.7143 (8.0878)	grad_norm 649.3448 (536.8678)	mem 14369MB
[2022-12-19 21:40:07 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [52/300][40/74]	eta 0:00:30 lr 0.000007	time 0.8697 (0.8839)	loss 10.1039 (8.2894)	grad_norm 293.9989 (568.1303)	mem 14369MB
[2022-12-19 21:40:16 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [52/300][50/74]	eta 0:00:21 lr 0.000007	time 0.8669 (0.8812)	loss 7.0057 (8.1755)	grad_norm 303.5235 (549.6223)	mem 14369MB
[2022-12-19 21:40:25 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [52/300][60/74]	eta 0:00:12 lr 0.000007	time 0.8618 (0.8789)	loss 7.8956 (8.1639)	grad_norm 680.8303 (564.8032)	mem 14369MB
[2022-12-19 21:40:34 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [52/300][70/74]	eta 0:00:03 lr 0.000007	time 0.8861 (0.8779)	loss 8.3120 (7.8817)	grad_norm 272.8377 (546.5706)	mem 14369MB
[2022-12-19 21:40:36 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 52 training takes 0:01:04
[2022-12-19 21:40:38 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [53/300][0/74]	eta 0:01:52 lr 0.000007	time 1.5183 (1.5183)	loss 10.8708 (10.8708)	grad_norm 931.6396 (931.6396)	mem 14369MB
[2022-12-19 21:40:46 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [53/300][10/74]	eta 0:00:59 lr 0.000007	time 0.8867 (0.9306)	loss 7.7223 (8.7161)	grad_norm 532.7055 (598.5903)	mem 14369MB
[2022-12-19 21:40:55 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [53/300][20/74]	eta 0:00:48 lr 0.000007	time 0.8776 (0.9012)	loss 6.3717 (7.5005)	grad_norm 367.9441 (575.2260)	mem 14369MB
[2022-12-19 21:41:04 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [53/300][30/74]	eta 0:00:39 lr 0.000007	time 0.8742 (0.8918)	loss 6.6997 (7.5415)	grad_norm 582.5780 (567.4552)	mem 14369MB
[2022-12-19 21:41:12 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [53/300][40/74]	eta 0:00:30 lr 0.000007	time 0.8638 (0.8858)	loss 7.6195 (7.5839)	grad_norm 260.5481 (578.2554)	mem 14369MB
[2022-12-19 21:41:21 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [53/300][50/74]	eta 0:00:21 lr 0.000007	time 0.8760 (0.8821)	loss 13.7416 (7.4906)	grad_norm 1088.7818 (571.2020)	mem 14369MB
[2022-12-19 21:41:30 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [53/300][60/74]	eta 0:00:12 lr 0.000007	time 0.8696 (0.8794)	loss 5.3882 (7.5284)	grad_norm 343.0717 (545.1907)	mem 14369MB
[2022-12-19 21:41:38 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [53/300][70/74]	eta 0:00:03 lr 0.000007	time 0.8689 (0.8775)	loss 14.3192 (7.9350)	grad_norm 804.8226 (561.6182)	mem 14369MB
[2022-12-19 21:41:41 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 53 training takes 0:01:04
[2022-12-19 21:41:42 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [54/300][0/74]	eta 0:01:52 lr 0.000007	time 1.5215 (1.5215)	loss 10.1347 (10.1347)	grad_norm 571.6533 (571.6533)	mem 14369MB
[2022-12-19 21:41:51 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [54/300][10/74]	eta 0:00:59 lr 0.000007	time 0.8783 (0.9258)	loss 9.1146 (8.4547)	grad_norm 619.8308 (539.0828)	mem 14369MB
[2022-12-19 21:42:00 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [54/300][20/74]	eta 0:00:48 lr 0.000007	time 0.8733 (0.8995)	loss 5.9886 (7.8085)	grad_norm 348.7394 (495.8270)	mem 14369MB
[2022-12-19 21:42:08 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [54/300][30/74]	eta 0:00:39 lr 0.000007	time 0.8622 (0.8884)	loss 7.1103 (7.6670)	grad_norm 831.4214 (532.1543)	mem 14369MB
[2022-12-19 21:42:17 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [54/300][40/74]	eta 0:00:30 lr 0.000007	time 0.8617 (0.8833)	loss 4.5130 (7.3852)	grad_norm 408.6720 (522.3316)	mem 14369MB
[2022-12-19 21:42:26 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [54/300][50/74]	eta 0:00:21 lr 0.000007	time 0.8861 (0.8809)	loss 5.8161 (7.3659)	grad_norm 359.5331 (529.1366)	mem 14369MB
[2022-12-19 21:42:35 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [54/300][60/74]	eta 0:00:12 lr 0.000007	time 0.8680 (0.8786)	loss 7.3480 (7.5314)	grad_norm 394.8906 (551.1982)	mem 14369MB
[2022-12-19 21:42:43 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [54/300][70/74]	eta 0:00:03 lr 0.000007	time 0.8707 (0.8766)	loss 11.4417 (7.6586)	grad_norm 704.0851 (576.0104)	mem 14369MB
[2022-12-19 21:42:46 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 54 training takes 0:01:04
[2022-12-19 21:42:47 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [55/300][0/74]	eta 0:01:54 lr 0.000007	time 1.5420 (1.5420)	loss 7.3989 (7.3989)	grad_norm 286.1071 (286.1071)	mem 14369MB
[2022-12-19 21:42:56 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [55/300][10/74]	eta 0:00:59 lr 0.000007	time 0.8673 (0.9337)	loss 10.7787 (7.8538)	grad_norm 568.5114 (542.2462)	mem 14369MB
[2022-12-19 21:43:05 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [55/300][20/74]	eta 0:00:48 lr 0.000007	time 0.8614 (0.9030)	loss 14.5076 (7.6070)	grad_norm 613.9756 (566.4686)	mem 14369MB
[2022-12-19 21:43:13 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [55/300][30/74]	eta 0:00:39 lr 0.000007	time 0.8711 (0.8922)	loss 6.6902 (7.4962)	grad_norm 589.1312 (572.6043)	mem 14369MB
[2022-12-19 21:43:22 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [55/300][40/74]	eta 0:00:30 lr 0.000007	time 0.8633 (0.8874)	loss 4.9441 (7.5463)	grad_norm 406.7992 (550.8290)	mem 14369MB
[2022-12-19 21:43:31 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [55/300][50/74]	eta 0:00:21 lr 0.000007	time 0.8639 (0.8845)	loss 5.9603 (7.4101)	grad_norm 541.4541 (541.0003)	mem 14369MB
[2022-12-19 21:43:40 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [55/300][60/74]	eta 0:00:12 lr 0.000007	time 0.8843 (0.8821)	loss 5.2103 (7.5032)	grad_norm 372.0447 (539.4315)	mem 14369MB
[2022-12-19 21:43:48 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [55/300][70/74]	eta 0:00:03 lr 0.000007	time 0.8737 (0.8807)	loss 5.6306 (7.6341)	grad_norm 551.5606 (547.3996)	mem 14369MB
[2022-12-19 21:43:51 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 55 training takes 0:01:05
[2022-12-19 21:43:52 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [56/300][0/74]	eta 0:01:51 lr 0.000007	time 1.5111 (1.5111)	loss 13.1333 (13.1333)	grad_norm 675.4852 (675.4852)	mem 14369MB
[2022-12-19 21:44:01 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [56/300][10/74]	eta 0:00:59 lr 0.000007	time 0.8691 (0.9257)	loss 6.2142 (7.7258)	grad_norm 289.8797 (552.3064)	mem 14369MB
[2022-12-19 21:44:10 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [56/300][20/74]	eta 0:00:48 lr 0.000007	time 0.8699 (0.8998)	loss 7.0697 (7.6467)	grad_norm 498.1832 (590.8643)	mem 14369MB
[2022-12-19 21:44:18 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [56/300][30/74]	eta 0:00:39 lr 0.000007	time 0.8800 (0.8898)	loss 4.9945 (7.4787)	grad_norm 328.7001 (556.0307)	mem 14369MB
[2022-12-19 21:44:27 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [56/300][40/74]	eta 0:00:30 lr 0.000007	time 0.8664 (0.8852)	loss 6.0935 (7.7286)	grad_norm 621.5578 (561.2265)	mem 14369MB
[2022-12-19 21:44:36 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [56/300][50/74]	eta 0:00:21 lr 0.000007	time 0.8688 (0.8834)	loss 10.6227 (7.7033)	grad_norm 1251.2935 (555.5830)	mem 14369MB
[2022-12-19 21:44:45 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [56/300][60/74]	eta 0:00:12 lr 0.000007	time 0.8756 (0.8813)	loss 6.1426 (7.8556)	grad_norm 555.8634 (574.3085)	mem 14369MB
[2022-12-19 21:44:53 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [56/300][70/74]	eta 0:00:03 lr 0.000007	time 0.8689 (0.8802)	loss 3.2457 (7.5001)	grad_norm 179.2839 (546.7100)	mem 14369MB
[2022-12-19 21:44:56 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 56 training takes 0:01:04
[2022-12-19 21:44:57 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [57/300][0/74]	eta 0:01:52 lr 0.000007	time 1.5138 (1.5138)	loss 6.7852 (6.7852)	grad_norm 742.7259 (742.7259)	mem 14369MB
[2022-12-19 21:45:06 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [57/300][10/74]	eta 0:00:59 lr 0.000007	time 0.8637 (0.9308)	loss 6.7352 (9.4972)	grad_norm 722.3038 (692.8847)	mem 14369MB
[2022-12-19 21:45:15 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [57/300][20/74]	eta 0:00:48 lr 0.000007	time 0.8712 (0.9004)	loss 6.1279 (8.4225)	grad_norm 445.7319 (618.0090)	mem 14369MB
[2022-12-19 21:45:23 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [57/300][30/74]	eta 0:00:39 lr 0.000007	time 0.8812 (0.8900)	loss 11.7824 (7.9648)	grad_norm 508.6276 (593.0377)	mem 14369MB
[2022-12-19 21:45:32 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [57/300][40/74]	eta 0:00:30 lr 0.000007	time 0.8551 (0.8841)	loss 5.3169 (7.6857)	grad_norm 639.9900 (588.0391)	mem 14369MB
[2022-12-19 21:45:41 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [57/300][50/74]	eta 0:00:21 lr 0.000007	time 0.8569 (0.8806)	loss 4.8612 (7.6599)	grad_norm 513.8305 (563.1576)	mem 14369MB
[2022-12-19 21:45:49 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [57/300][60/74]	eta 0:00:12 lr 0.000007	time 0.8665 (0.8789)	loss 4.9030 (7.7536)	grad_norm 505.0628 (563.2327)	mem 14369MB
[2022-12-19 21:45:58 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [57/300][70/74]	eta 0:00:03 lr 0.000007	time 0.8611 (0.8773)	loss 7.1714 (7.7177)	grad_norm 697.9587 (567.9637)	mem 14369MB
[2022-12-19 21:46:01 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 57 training takes 0:01:04
[2022-12-19 21:46:02 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [58/300][0/74]	eta 0:01:51 lr 0.000007	time 1.5072 (1.5072)	loss 6.9426 (6.9426)	grad_norm 793.4446 (793.4446)	mem 14369MB
[2022-12-19 21:46:11 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [58/300][10/74]	eta 0:00:59 lr 0.000007	time 0.8623 (0.9233)	loss 7.4781 (6.8381)	grad_norm 983.9950 (617.7030)	mem 14369MB
[2022-12-19 21:46:20 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [58/300][20/74]	eta 0:00:48 lr 0.000007	time 0.8715 (0.8999)	loss 8.1583 (6.7929)	grad_norm 530.1631 (591.2183)	mem 14369MB
[2022-12-19 21:46:28 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [58/300][30/74]	eta 0:00:39 lr 0.000007	time 0.8790 (0.8902)	loss 13.1877 (7.0495)	grad_norm 662.0405 (609.8678)	mem 14369MB
[2022-12-19 21:46:37 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [58/300][40/74]	eta 0:00:30 lr 0.000007	time 0.8684 (0.8858)	loss 5.6647 (6.8877)	grad_norm 559.2551 (581.6372)	mem 14369MB
[2022-12-19 21:46:46 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [58/300][50/74]	eta 0:00:21 lr 0.000007	time 0.8772 (0.8820)	loss 11.3682 (7.3667)	grad_norm 496.6222 (580.5392)	mem 14369MB
[2022-12-19 21:46:54 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [58/300][60/74]	eta 0:00:12 lr 0.000007	time 0.8691 (0.8795)	loss 5.1110 (7.2594)	grad_norm 201.8993 (575.8978)	mem 14369MB
[2022-12-19 21:47:03 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [58/300][70/74]	eta 0:00:03 lr 0.000007	time 0.8733 (0.8783)	loss 5.5734 (7.3672)	grad_norm 424.0574 (586.0709)	mem 14369MB
[2022-12-19 21:47:06 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 58 training takes 0:01:04
[2022-12-19 21:47:07 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [59/300][0/74]	eta 0:01:51 lr 0.000007	time 1.5083 (1.5083)	loss 7.6802 (7.6802)	grad_norm 362.5297 (362.5297)	mem 14369MB
[2022-12-19 21:47:16 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [59/300][10/74]	eta 0:00:59 lr 0.000007	time 0.8697 (0.9255)	loss 5.6296 (7.7166)	grad_norm 525.3956 (457.6425)	mem 14369MB
[2022-12-19 21:47:24 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [59/300][20/74]	eta 0:00:48 lr 0.000007	time 0.8623 (0.8981)	loss 5.3057 (7.2590)	grad_norm 394.4038 (410.9918)	mem 14369MB
[2022-12-19 21:47:33 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [59/300][30/74]	eta 0:00:39 lr 0.000007	time 0.8594 (0.8888)	loss 9.3944 (7.1468)	grad_norm 1210.0059 (476.8515)	mem 14369MB
[2022-12-19 21:47:42 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [59/300][40/74]	eta 0:00:30 lr 0.000007	time 0.8607 (0.8832)	loss 5.9117 (7.0264)	grad_norm 664.4137 (478.5089)	mem 14369MB
[2022-12-19 21:47:50 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [59/300][50/74]	eta 0:00:21 lr 0.000007	time 0.8709 (0.8807)	loss 4.1902 (7.0223)	grad_norm 358.0430 (475.2183)	mem 14369MB
[2022-12-19 21:47:59 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [59/300][60/74]	eta 0:00:12 lr 0.000007	time 0.8841 (0.8793)	loss 7.4321 (7.2771)	grad_norm 499.3794 (493.0199)	mem 14369MB
[2022-12-19 21:48:08 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [59/300][70/74]	eta 0:00:03 lr 0.000007	time 0.8636 (0.8777)	loss 7.6997 (7.1034)	grad_norm 768.9167 (507.8602)	mem 14369MB
[2022-12-19 21:48:10 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 59 training takes 0:01:04
[2022-12-19 21:48:12 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [60/300][0/74]	eta 0:01:51 lr 0.000007	time 1.5006 (1.5006)	loss 6.8332 (6.8332)	grad_norm 863.6463 (863.6463)	mem 14369MB
[2022-12-19 21:48:21 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [60/300][10/74]	eta 0:00:59 lr 0.000007	time 0.8632 (0.9243)	loss 8.6700 (6.7063)	grad_norm 484.3446 (509.2738)	mem 14369MB
[2022-12-19 21:48:29 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [60/300][20/74]	eta 0:00:48 lr 0.000007	time 0.8689 (0.8983)	loss 6.0738 (7.0359)	grad_norm 416.0778 (456.7459)	mem 14369MB
[2022-12-19 21:48:38 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [60/300][30/74]	eta 0:00:39 lr 0.000007	time 0.8760 (0.8884)	loss 4.6271 (6.7633)	grad_norm 300.8118 (428.6054)	mem 14369MB
[2022-12-19 21:48:47 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [60/300][40/74]	eta 0:00:30 lr 0.000007	time 0.8736 (0.8832)	loss 9.4464 (6.9533)	grad_norm 957.2859 (473.1772)	mem 14369MB
[2022-12-19 21:48:55 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [60/300][50/74]	eta 0:00:21 lr 0.000007	time 0.8687 (0.8813)	loss 6.6087 (6.8247)	grad_norm 498.4307 (482.7873)	mem 14369MB
[2022-12-19 21:49:04 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [60/300][60/74]	eta 0:00:12 lr 0.000007	time 0.8718 (0.8791)	loss 5.8561 (6.9095)	grad_norm 366.4969 (482.1642)	mem 14369MB
[2022-12-19 21:49:13 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [60/300][70/74]	eta 0:00:03 lr 0.000007	time 0.8669 (0.8777)	loss 7.3565 (6.7985)	grad_norm 553.1697 (479.7373)	mem 14369MB
[2022-12-19 21:49:15 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 60 training takes 0:01:04
[2022-12-19 21:49:15 RepVGGplus-tinyism] (helpers.py 207): INFO ./output/repvggplus/RepVGGplus-tinyism/default/ckpt_epoch_60.pth saving......
[2022-12-19 21:49:16 RepVGGplus-tinyism] (helpers.py 209): INFO ./output/repvggplus/RepVGGplus-tinyism/default/ckpt_epoch_60.pth saved !!!
[2022-12-19 21:49:18 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [61/300][0/74]	eta 0:01:52 lr 0.000007	time 1.5263 (1.5263)	loss 7.8368 (7.8368)	grad_norm 482.4309 (482.4309)	mem 14369MB
[2022-12-19 21:49:27 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [61/300][10/74]	eta 0:00:59 lr 0.000007	time 0.8697 (0.9260)	loss 5.7226 (5.8089)	grad_norm 420.1265 (423.5073)	mem 14369MB
[2022-12-19 21:49:35 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [61/300][20/74]	eta 0:00:48 lr 0.000007	time 0.8569 (0.8959)	loss 4.2320 (5.9596)	grad_norm 551.9657 (456.6089)	mem 14369MB
[2022-12-19 21:49:44 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [61/300][30/74]	eta 0:00:39 lr 0.000007	time 0.8653 (0.8866)	loss 8.8518 (5.9627)	grad_norm 832.3075 (458.4079)	mem 14369MB
[2022-12-19 21:49:53 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [61/300][40/74]	eta 0:00:29 lr 0.000007	time 0.8672 (0.8823)	loss 6.3201 (6.2911)	grad_norm 672.3034 (489.6277)	mem 14369MB
[2022-12-19 21:50:01 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [61/300][50/74]	eta 0:00:21 lr 0.000007	time 0.8851 (0.8806)	loss 4.5864 (6.6438)	grad_norm 243.0077 (480.3738)	mem 14369MB
[2022-12-19 21:50:10 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [61/300][60/74]	eta 0:00:12 lr 0.000007	time 0.8759 (0.8785)	loss 9.0807 (6.7459)	grad_norm 508.5439 (478.2149)	mem 14369MB
[2022-12-19 21:50:19 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [61/300][70/74]	eta 0:00:03 lr 0.000007	time 0.8659 (0.8767)	loss 4.1624 (6.8184)	grad_norm 469.6818 (492.3851)	mem 14369MB
[2022-12-19 21:50:21 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 61 training takes 0:01:04
[2022-12-19 21:50:23 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [62/300][0/74]	eta 0:01:52 lr 0.000007	time 1.5157 (1.5157)	loss 4.9956 (4.9956)	grad_norm 447.6733 (447.6733)	mem 14369MB
[2022-12-19 21:50:31 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [62/300][10/74]	eta 0:00:59 lr 0.000007	time 0.8693 (0.9251)	loss 5.3631 (5.5060)	grad_norm 558.8705 (530.1363)	mem 14369MB
[2022-12-19 21:50:40 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [62/300][20/74]	eta 0:00:48 lr 0.000007	time 0.8640 (0.8975)	loss 10.8030 (6.3935)	grad_norm 596.8459 (532.7352)	mem 14369MB
[2022-12-19 21:50:49 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [62/300][30/74]	eta 0:00:39 lr 0.000007	time 0.8750 (0.8884)	loss 10.4065 (6.2893)	grad_norm 569.3175 (501.5054)	mem 14369MB
[2022-12-19 21:50:57 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [62/300][40/74]	eta 0:00:30 lr 0.000007	time 0.8627 (0.8828)	loss 4.9437 (6.5322)	grad_norm 434.7643 (504.9781)	mem 14369MB
[2022-12-19 21:51:06 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [62/300][50/74]	eta 0:00:21 lr 0.000007	time 0.8599 (0.8792)	loss 5.4003 (6.5081)	grad_norm 554.9009 (505.5868)	mem 14369MB
[2022-12-19 21:51:15 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [62/300][60/74]	eta 0:00:12 lr 0.000007	time 0.8681 (0.8780)	loss 7.2403 (6.7416)	grad_norm 510.1445 (500.8444)	mem 14369MB
[2022-12-19 21:51:23 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [62/300][70/74]	eta 0:00:03 lr 0.000007	time 0.8611 (0.8767)	loss 8.6844 (6.6345)	grad_norm 703.9680 (488.2051)	mem 14369MB
[2022-12-19 21:51:26 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 62 training takes 0:01:04
[2022-12-19 21:51:28 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [63/300][0/74]	eta 0:01:51 lr 0.000007	time 1.5057 (1.5057)	loss 7.7687 (7.7687)	grad_norm 309.4118 (309.4118)	mem 14369MB
[2022-12-19 21:51:36 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [63/300][10/74]	eta 0:00:59 lr 0.000007	time 0.8821 (0.9284)	loss 5.1804 (6.7711)	grad_norm 421.9787 (407.3687)	mem 14369MB
[2022-12-19 21:51:45 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [63/300][20/74]	eta 0:00:48 lr 0.000007	time 0.8826 (0.9011)	loss 4.6367 (6.2685)	grad_norm 308.6697 (413.3655)	mem 14369MB
[2022-12-19 21:51:54 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [63/300][30/74]	eta 0:00:39 lr 0.000007	time 0.8643 (0.8912)	loss 5.4812 (6.3488)	grad_norm 564.7373 (457.9752)	mem 14369MB
[2022-12-19 21:52:02 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [63/300][40/74]	eta 0:00:30 lr 0.000007	time 0.8659 (0.8856)	loss 6.5238 (6.4080)	grad_norm 550.8227 (477.7572)	mem 14369MB
[2022-12-19 21:52:11 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [63/300][50/74]	eta 0:00:21 lr 0.000007	time 0.8589 (0.8823)	loss 8.3316 (6.5477)	grad_norm 1176.5142 (502.5149)	mem 14369MB
[2022-12-19 21:52:20 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [63/300][60/74]	eta 0:00:12 lr 0.000007	time 0.8609 (0.8801)	loss 5.8116 (6.3403)	grad_norm 598.2565 (483.3944)	mem 14369MB
[2022-12-19 21:52:28 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [63/300][70/74]	eta 0:00:03 lr 0.000007	time 0.8745 (0.8785)	loss 5.8856 (6.5518)	grad_norm 419.6284 (486.2891)	mem 14369MB
[2022-12-19 21:52:31 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 63 training takes 0:01:04
[2022-12-19 21:52:32 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [64/300][0/74]	eta 0:01:51 lr 0.000007	time 1.5106 (1.5106)	loss 6.9856 (6.9856)	grad_norm 265.1358 (265.1358)	mem 14369MB
[2022-12-19 21:52:41 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [64/300][10/74]	eta 0:00:59 lr 0.000007	time 0.8602 (0.9247)	loss 4.7810 (6.9183)	grad_norm 438.9424 (422.9553)	mem 14369MB
[2022-12-19 21:52:50 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [64/300][20/74]	eta 0:00:48 lr 0.000007	time 0.8664 (0.8985)	loss 5.3543 (6.3201)	grad_norm 357.5994 (422.6169)	mem 14369MB
[2022-12-19 21:52:58 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [64/300][30/74]	eta 0:00:39 lr 0.000007	time 0.8684 (0.8884)	loss 5.1646 (6.4592)	grad_norm 304.1827 (434.9803)	mem 14369MB
[2022-12-19 21:53:07 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [64/300][40/74]	eta 0:00:30 lr 0.000007	time 0.8649 (0.8832)	loss 11.4834 (6.2576)	grad_norm 382.7330 (428.1885)	mem 14369MB
[2022-12-19 21:53:16 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [64/300][50/74]	eta 0:00:21 lr 0.000007	time 0.8642 (0.8801)	loss 6.2124 (6.3841)	grad_norm 555.4264 (445.0319)	mem 14369MB
[2022-12-19 21:53:24 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [64/300][60/74]	eta 0:00:12 lr 0.000007	time 0.8650 (0.8778)	loss 11.0633 (6.3656)	grad_norm 679.9076 (447.7996)	mem 14369MB
[2022-12-19 21:53:33 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [64/300][70/74]	eta 0:00:03 lr 0.000007	time 0.8610 (0.8768)	loss 9.1350 (6.4444)	grad_norm 763.2634 (450.6494)	mem 14369MB
[2022-12-19 21:53:36 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 64 training takes 0:01:04
[2022-12-19 21:53:37 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [65/300][0/74]	eta 0:01:52 lr 0.000007	time 1.5143 (1.5143)	loss 8.5787 (8.5787)	grad_norm 651.8219 (651.8219)	mem 14369MB
[2022-12-19 21:53:46 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [65/300][10/74]	eta 0:00:59 lr 0.000007	time 0.8821 (0.9271)	loss 10.8738 (6.0392)	grad_norm 279.0334 (364.3182)	mem 14369MB
[2022-12-19 21:53:55 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [65/300][20/74]	eta 0:00:48 lr 0.000007	time 0.8764 (0.8974)	loss 8.5874 (6.4323)	grad_norm 546.7691 (446.6972)	mem 14369MB
[2022-12-19 21:54:03 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [65/300][30/74]	eta 0:00:39 lr 0.000007	time 0.8652 (0.8875)	loss 3.8959 (7.2943)	grad_norm 298.7723 (444.5844)	mem 14369MB
[2022-12-19 21:54:12 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [65/300][40/74]	eta 0:00:30 lr 0.000007	time 0.8732 (0.8829)	loss 8.2493 (7.1211)	grad_norm 505.0321 (449.1239)	mem 14369MB
[2022-12-19 21:54:21 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [65/300][50/74]	eta 0:00:21 lr 0.000007	time 0.8669 (0.8795)	loss 4.5386 (6.7749)	grad_norm 310.8190 (440.0516)	mem 14369MB
[2022-12-19 21:54:29 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [65/300][60/74]	eta 0:00:12 lr 0.000007	time 0.8873 (0.8778)	loss 4.4524 (6.6126)	grad_norm 233.4548 (434.8745)	mem 14369MB
[2022-12-19 21:54:38 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [65/300][70/74]	eta 0:00:03 lr 0.000007	time 0.8625 (0.8758)	loss 4.3225 (6.3451)	grad_norm 485.2905 (433.5827)	mem 14369MB
[2022-12-19 21:54:40 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 65 training takes 0:01:04
[2022-12-19 21:54:42 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [66/300][0/74]	eta 0:01:51 lr 0.000007	time 1.5109 (1.5109)	loss 4.6885 (4.6885)	grad_norm 363.5725 (363.5725)	mem 14369MB
[2022-12-19 21:54:51 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [66/300][10/74]	eta 0:00:59 lr 0.000007	time 0.8726 (0.9285)	loss 7.9430 (6.5635)	grad_norm 457.3024 (441.2637)	mem 14369MB
[2022-12-19 21:54:59 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [66/300][20/74]	eta 0:00:48 lr 0.000007	time 0.8823 (0.9008)	loss 12.3012 (6.4793)	grad_norm 459.8948 (439.3012)	mem 14369MB
[2022-12-19 21:55:08 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [66/300][30/74]	eta 0:00:39 lr 0.000007	time 0.8628 (0.8911)	loss 5.7922 (6.1267)	grad_norm 453.0990 (421.7263)	mem 14369MB
[2022-12-19 21:55:17 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [66/300][40/74]	eta 0:00:30 lr 0.000007	time 0.8741 (0.8855)	loss 5.0561 (6.0260)	grad_norm 352.3796 (425.5697)	mem 14369MB
[2022-12-19 21:55:25 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [66/300][50/74]	eta 0:00:21 lr 0.000007	time 0.8796 (0.8821)	loss 5.5238 (6.2158)	grad_norm 343.5062 (453.6233)	mem 14369MB
[2022-12-19 21:55:34 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [66/300][60/74]	eta 0:00:12 lr 0.000007	time 0.8603 (0.8792)	loss 6.6604 (6.1668)	grad_norm 491.0611 (454.7649)	mem 14369MB
[2022-12-19 21:55:43 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [66/300][70/74]	eta 0:00:03 lr 0.000007	time 0.8605 (0.8778)	loss 6.2268 (6.2068)	grad_norm 670.4507 (447.2045)	mem 14369MB
[2022-12-19 21:55:45 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 66 training takes 0:01:04
[2022-12-19 21:55:47 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [67/300][0/74]	eta 0:01:51 lr 0.000007	time 1.5040 (1.5040)	loss 3.5428 (3.5428)	grad_norm 384.0219 (384.0219)	mem 14369MB
[2022-12-19 21:55:55 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [67/300][10/74]	eta 0:00:59 lr 0.000007	time 0.8661 (0.9271)	loss 5.5399 (6.0795)	grad_norm 267.4820 (381.7595)	mem 14369MB
[2022-12-19 21:56:04 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [67/300][20/74]	eta 0:00:48 lr 0.000007	time 0.8615 (0.8976)	loss 6.1097 (6.0203)	grad_norm 581.1825 (423.9647)	mem 14369MB
[2022-12-19 21:56:13 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [67/300][30/74]	eta 0:00:39 lr 0.000007	time 0.8559 (0.8878)	loss 5.1987 (5.8986)	grad_norm 535.4917 (412.1806)	mem 14369MB
[2022-12-19 21:56:21 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [67/300][40/74]	eta 0:00:29 lr 0.000007	time 0.8727 (0.8823)	loss 4.6987 (5.9890)	grad_norm 361.6890 (430.4776)	mem 14369MB
[2022-12-19 21:56:30 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [67/300][50/74]	eta 0:00:21 lr 0.000007	time 0.8726 (0.8794)	loss 4.3550 (5.7684)	grad_norm 294.9490 (420.3338)	mem 14369MB
[2022-12-19 21:56:39 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [67/300][60/74]	eta 0:00:12 lr 0.000007	time 0.8588 (0.8774)	loss 3.2773 (5.8406)	grad_norm 283.1468 (410.0020)	mem 14369MB
[2022-12-19 21:56:47 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [67/300][70/74]	eta 0:00:03 lr 0.000007	time 0.8657 (0.8763)	loss 6.0699 (5.8866)	grad_norm 439.1261 (405.3329)	mem 14369MB
[2022-12-19 21:56:50 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 67 training takes 0:01:04
[2022-12-19 21:56:52 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [68/300][0/74]	eta 0:01:52 lr 0.000007	time 1.5172 (1.5172)	loss 5.9779 (5.9779)	grad_norm 729.8564 (729.8564)	mem 14369MB
[2022-12-19 21:57:00 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [68/300][10/74]	eta 0:00:59 lr 0.000007	time 0.8706 (0.9252)	loss 7.3514 (5.8002)	grad_norm 586.2014 (467.7431)	mem 14369MB
[2022-12-19 21:57:09 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [68/300][20/74]	eta 0:00:48 lr 0.000007	time 0.8565 (0.8966)	loss 8.8031 (6.3672)	grad_norm 941.9510 (507.3277)	mem 14369MB
[2022-12-19 21:57:17 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [68/300][30/74]	eta 0:00:39 lr 0.000007	time 0.8651 (0.8871)	loss 8.4052 (6.3075)	grad_norm 410.4395 (490.7351)	mem 14369MB
[2022-12-19 21:57:26 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [68/300][40/74]	eta 0:00:29 lr 0.000007	time 0.8665 (0.8819)	loss 4.0509 (5.9838)	grad_norm 287.7876 (457.8675)	mem 14369MB
[2022-12-19 21:57:35 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [68/300][50/74]	eta 0:00:21 lr 0.000007	time 0.8715 (0.8810)	loss 4.4722 (5.9356)	grad_norm 304.8957 (456.9980)	mem 14369MB
[2022-12-19 21:57:44 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [68/300][60/74]	eta 0:00:12 lr 0.000007	time 0.8633 (0.8782)	loss 9.0668 (5.9351)	grad_norm 544.3128 (443.2287)	mem 14369MB
[2022-12-19 21:57:52 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [68/300][70/74]	eta 0:00:03 lr 0.000007	time 0.8632 (0.8773)	loss 7.1211 (6.0700)	grad_norm 349.6894 (458.3913)	mem 14369MB
[2022-12-19 21:57:55 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 68 training takes 0:01:04
[2022-12-19 21:57:56 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [69/300][0/74]	eta 0:01:52 lr 0.000007	time 1.5248 (1.5248)	loss 4.6979 (4.6979)	grad_norm 246.7806 (246.7806)	mem 14369MB
[2022-12-19 21:58:05 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [69/300][10/74]	eta 0:00:59 lr 0.000007	time 0.8663 (0.9289)	loss 5.3649 (5.2959)	grad_norm 407.9768 (428.8621)	mem 14369MB
[2022-12-19 21:58:14 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [69/300][20/74]	eta 0:00:48 lr 0.000007	time 0.8655 (0.8994)	loss 5.9608 (5.6938)	grad_norm 552.6137 (417.6439)	mem 14369MB
[2022-12-19 21:58:22 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [69/300][30/74]	eta 0:00:39 lr 0.000007	time 0.8639 (0.8891)	loss 4.2349 (5.6900)	grad_norm 374.2150 (409.3900)	mem 14369MB
[2022-12-19 21:58:31 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [69/300][40/74]	eta 0:00:30 lr 0.000007	time 0.8612 (0.8840)	loss 4.8211 (5.6963)	grad_norm 517.6143 (408.9412)	mem 14369MB
[2022-12-19 21:58:40 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [69/300][50/74]	eta 0:00:21 lr 0.000007	time 0.8562 (0.8804)	loss 4.2673 (5.8162)	grad_norm 534.2354 (428.4966)	mem 14369MB
[2022-12-19 21:58:48 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [69/300][60/74]	eta 0:00:12 lr 0.000007	time 0.8656 (0.8787)	loss 6.4784 (5.9527)	grad_norm 719.6592 (441.6765)	mem 14369MB
[2022-12-19 21:58:57 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [69/300][70/74]	eta 0:00:03 lr 0.000007	time 0.8820 (0.8773)	loss 7.1018 (6.0746)	grad_norm 398.1079 (441.2259)	mem 14369MB
[2022-12-19 21:59:00 RepVGGplus-tinyism] (train_repvgg.py 217): INFO EPOCH 69 training takes 0:01:04
[2022-12-19 21:59:01 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [70/300][0/74]	eta 0:01:53 lr 0.000007	time 1.5300 (1.5300)	loss 4.8955 (4.8955)	grad_norm 352.0659 (352.0659)	mem 14369MB
[2022-12-19 21:59:10 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [70/300][10/74]	eta 0:00:59 lr 0.000007	time 0.8672 (0.9263)	loss 5.2747 (5.5833)	grad_norm 306.0548 (392.2047)	mem 14369MB
[2022-12-19 21:59:18 RepVGGplus-tinyism] (train_repvgg.py 210): INFO Train: [70/300][20/74]	eta 0:00:48 lr 0.000007	time 0.8705 (0.8989)	loss 7.2712 (5.6331)	grad_norm 377.4511 (381.6356)	mem 14369MB
