{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "import torch\n",
    "sys.path.append(os.path.abspath('../models'))\n",
    "from tinyism import tinyModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "posencoding=True\n",
    "model=tinyModel(posEncoding=posencoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tinyModel(\n",
       "  (backbone): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act1): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (global_pool): SelectAdaptivePool2d (pool_type=, flatten=Identity())\n",
       "    (fc): Identity()\n",
       "  )\n",
       "  (class_head): Sequential(\n",
       "    (0): Conv2d(514, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (conv_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu_0): ReLU()\n",
       "    (conv_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu_1): ReLU()\n",
       "    (conv_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu_2): ReLU()\n",
       "    (conv_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu_3): ReLU()\n",
       "    (conv_4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu_4): ReLU()\n",
       "    (cls): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (box_head): Sequential(\n",
       "    (0): Conv2d(514, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (conv_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu_0): ReLU()\n",
       "    (conv_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu_1): ReLU()\n",
       "    (conv_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu_2): ReLU()\n",
       "    (conv_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu_3): ReLU()\n",
       "    (conv_4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu_4): ReLU()\n",
       "    (conv_5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu_5): ReLU()\n",
       "  )\n",
       "  (box): Conv2d(256, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (offset_head): Sequential(\n",
       "    (0): Conv2d(514, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (conv_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu_0): ReLU()\n",
       "    (conv_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu_1): ReLU()\n",
       "    (conv_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu_2): ReLU()\n",
       "    (conv_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu_3): ReLU()\n",
       "    (conv_4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu_4): ReLU()\n",
       "    (conv_5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu_5): ReLU()\n",
       "  )\n",
       "  (offset): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (seg_head): Sequential(\n",
       "    (0): Conv2d(5, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (conv_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu_0): ReLU()\n",
       "    (conv_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu_1): ReLU()\n",
       "    (conv_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu_2): ReLU()\n",
       "    (conv_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu_3): ReLU()\n",
       "    (conv_4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu_4): ReLU()\n",
       "    (conv_5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu_5): ReLU()\n",
       "  )\n",
       "  (seg): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " weight=\"../weights/resnet18_inst.pth\"\n",
    " model.load_state_dict(torch.load(weight,map_location=torch.device('cpu')))\n",
    " model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "    sys.path.append(os.path.abspath('../datasets'))\n",
    "    from data_loader import preprocess\n",
    "    import cv2\n",
    "    img=cv2.imread(\"../test.png\")\n",
    "    img=cv2.resize(img,(512,512))\n",
    "    x=preprocess((torch.from_numpy(img).float())).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 512, 512]), torch.Size([4]), torch.Size([4, 1, 64, 64]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## ONNX conversion\n",
    "y1,y2=model(x)\n",
    "x.shape,y1.shape,y2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/einops/einops.py:204: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  inferred_length: int = length // known_product\n",
      "/home/ec2-user/SageMaker/tiny-instance-segmentation/models/tinyism.py:42: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "  if len(bidx)>maxdets:\n",
      "/home/ec2-user/SageMaker/tiny-instance-segmentation/models/tinyism.py:52: TracerWarning: Converting a tensor to a NumPy array might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  pred_boxes=pred_boxes[bidx,idx,jdx,:].detach().numpy()\n",
      "/home/ec2-user/SageMaker/tiny-instance-segmentation/models/tinyism.py:53: TracerWarning: Converting a tensor to a NumPy array might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  pred_offs=pred_offs[bidx,idx,jdx,:].detach().numpy()\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/onnx/symbolic_opset9.py:2815: UserWarning: Exporting aten::index operator of advanced indexing in opset 12 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
      "  warnings.warn(\"Exporting aten::index operator of advanced indexing in opset \" +\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torchvision/ops/_register_onnx_ops.py:36: UserWarning: ONNX doesn't support negative sampling ratio,therefore is is set to 0 in order to be exported.\n",
      "  warnings.warn(\"ONNX doesn't support negative sampling ratio,\"\n"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(model,               # model being run\n",
    "                  x,                         # model input (or a tuple for multiple inputs)\n",
    "                  \"tinynet.onnx\",   # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=12,          # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = ['input'],   # the model's input names\n",
    "                  output_names = ['output'], # the model's output names\n",
    "                  dynamic_axes={'input' : {0 : 'batch_size',1: 'instances'},    # variable length axes\n",
    "                                'output' : {0 : 'batch_size',1: 'outputinstances'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check conversion\n",
    "!pip install onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "ort_session = onnxruntime.InferenceSession(\"tinynet.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)}\n",
    "ort_outs = ort_session.run(None, ort_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 512, 512)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_image=to_numpy(x)\n",
    "input_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4,), (4, 1, 64, 64), (4,), (4, 1, 64, 64))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1=to_numpy(y1)\n",
    "y2=to_numpy(y2)\n",
    "ort_outs[0].shape,ort_outs[1].shape,y1.shape,y2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.16491759, 0.16338453, 0.16040626, ..., 0.16071782,\n",
       "          0.16183221, 0.15289238],\n",
       "         [0.16723716, 0.17236674, 0.17438   , ..., 0.16114116,\n",
       "          0.16439745, 0.15792394],\n",
       "         [0.16422993, 0.16862863, 0.17395005, ..., 0.16700858,\n",
       "          0.16054675, 0.15998146],\n",
       "         ...,\n",
       "         [0.13980982, 0.15187079, 0.17807859, ..., 0.15225616,\n",
       "          0.16712594, 0.17388237],\n",
       "         [0.13712534, 0.14206952, 0.16376051, ..., 0.16094291,\n",
       "          0.15684304, 0.15327564],\n",
       "         [0.13647446, 0.142115  , 0.14452785, ..., 0.17303154,\n",
       "          0.18076345, 0.16862842]]],\n",
       "\n",
       "\n",
       "       [[[0.15834999, 0.15854937, 0.15511957, ..., 0.18060303,\n",
       "          0.1717546 , 0.17188361],\n",
       "         [0.16032788, 0.16409266, 0.16513076, ..., 0.18978205,\n",
       "          0.18537393, 0.1771403 ],\n",
       "         [0.16137469, 0.1650309 , 0.17318162, ..., 0.17538962,\n",
       "          0.17245567, 0.1754936 ],\n",
       "         ...,\n",
       "         [0.13875446, 0.14308533, 0.14854527, ..., 0.14195853,\n",
       "          0.11302778, 0.13664228],\n",
       "         [0.1446408 , 0.13824084, 0.141303  , ..., 0.14757651,\n",
       "          0.12759796, 0.14785293],\n",
       "         [0.15976804, 0.14485723, 0.15188345, ..., 0.13562503,\n",
       "          0.12470278, 0.1412639 ]]],\n",
       "\n",
       "\n",
       "       [[[0.1534428 , 0.14963686, 0.14561054, ..., 0.16346207,\n",
       "          0.1447874 , 0.14734924],\n",
       "         [0.15824333, 0.15853024, 0.14748809, ..., 0.16250175,\n",
       "          0.15401182, 0.15200514],\n",
       "         [0.1580768 , 0.16055033, 0.16207635, ..., 0.15787363,\n",
       "          0.15474129, 0.16532084],\n",
       "         ...,\n",
       "         [0.12190375, 0.12320912, 0.13751683, ..., 0.17053473,\n",
       "          0.16419798, 0.16142884],\n",
       "         [0.13885486, 0.12917188, 0.13256052, ..., 0.17304367,\n",
       "          0.17083663, 0.15809354],\n",
       "         [0.13875869, 0.13056225, 0.13350934, ..., 0.15894464,\n",
       "          0.15185374, 0.15516758]]],\n",
       "\n",
       "\n",
       "       [[[0.16505748, 0.16169524, 0.16166434, ..., 0.13951287,\n",
       "          0.12418666, 0.12090528],\n",
       "         [0.16733205, 0.16884202, 0.16816783, ..., 0.13731766,\n",
       "          0.1335221 , 0.1336579 ],\n",
       "         [0.16197133, 0.16211885, 0.16350636, ..., 0.14211416,\n",
       "          0.13887444, 0.13817787],\n",
       "         ...,\n",
       "         [0.12350887, 0.13700655, 0.15530065, ..., 0.96282685,\n",
       "          0.96360743, 0.9151092 ],\n",
       "         [0.12390599, 0.13189957, 0.14458817, ..., 0.9465244 ,\n",
       "          0.56402797, 0.21547908],\n",
       "         [0.12914556, 0.13165405, 0.1322732 , ..., 0.17156258,\n",
       "          0.15231377, 0.13580528]]]], dtype=float32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ort_outs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.allclose(y1, ort_outs[0], rtol=1e-03, atol=1e-05),np.allclose(y2, ort_outs[1], rtol=1e-03, atol=1e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Openvio conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of numpy.core.multiarray failed: Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 604, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/numpy/core/multiarray.py\", line 16, in <module>\n",
      "    from ._multiarray_umath import (\n",
      "ImportError: cannot import name '_fastCopyAndTranspose' from 'numpy.core._multiarray_umath' (/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-x86_64-linux-gnu.so)\n",
      "]\n",
      "[autoreload of numpy.core.numeric failed: Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 604, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/numpy/core/numeric.py\", line 10, in <module>\n",
      "    from .multiarray import (\n",
      "ImportError: cannot import name '_fastCopyAndTranspose' from 'numpy.core.multiarray' (/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/numpy/core/multiarray.py)\n",
      "]\n",
      "[autoreload of numpy.linalg.linalg failed: Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 604, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/numpy/linalg/linalg.py\", line 85, in <module>\n",
      "    _linalg_error_extobj = _determine_error_states()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/numpy/linalg/linalg.py\", line 78, in _determine_error_states\n",
      "    with errstate(invalid='call', over='ignore',\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/numpy/core/_ufunc_config.py\", line 433, in __enter__\n",
      "ValueError: Only callable can be used as callback\n",
      "]\n",
      "[autoreload of numpy.matrixlib failed: Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 604, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/numpy/matrixlib/__init__.py\", line 6, in <module>\n",
      "    __all__ = defmatrix.__all__\n",
      "NameError: name 'defmatrix' is not defined\n",
      "]\n",
      "[autoreload of numpy.lib failed: Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 604, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/numpy/lib/__init__.py\", line 44, in <module>\n",
      "    __all__ += type_check.__all__\n",
      "NameError: name 'type_check' is not defined\n",
      "]\n",
      "[autoreload of numpy failed: Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 604, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/numpy/__init__.py\", line 191, in <module>\n",
      "    core.numerictypes.typeDict,\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/numpy/core/__init__.py\", line 161, in __getattr__\n",
      "    raise AttributeError(f\"Module {__name__!r} has no attribute {name!r}\")\n",
      "AttributeError: Module 'numpy.core' has no attribute 'numerictypes'\n",
      "]\n",
      "[autoreload of pandas._config.config failed: Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 410, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 317, in update_class\n",
      "    update_instances(old, new)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 280, in update_instances\n",
      "    ref.__class__ = new\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/pandas/_config/config.py\", line 208, in __setattr__\n",
      "    raise OptionError(\"You can only set the value of existing options\")\n",
      "pandas._config.config.OptionError: 'You can only set the value of existing options'\n",
      "]\n",
      "[autoreload of pandas._testing failed: Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 604, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/pandas/_testing/__init__.py\", line 948, in <module>\n",
      "    cython_table = pd.core.common._cython_table.items()\n",
      "AttributeError: module 'pandas.core' has no attribute 'common'\n",
      "]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openvino-dev in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (2022.3.0)\n",
      "Requirement already satisfied: defusedxml>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from openvino-dev) (0.7.1)\n",
      "Requirement already satisfied: openvino==2022.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from openvino-dev) (2022.3.0)\n",
      "Requirement already satisfied: jstyleson>=0.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from openvino-dev) (0.0.2)\n",
      "Requirement already satisfied: opencv-python>=4.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from openvino-dev) (4.5.1.48)\n",
      "Requirement already satisfied: pillow>=8.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from openvino-dev) (9.0.1)\n",
      "Requirement already satisfied: texttable>=1.6.3 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from openvino-dev) (1.6.7)\n",
      "Requirement already satisfied: networkx<=2.8.8 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from openvino-dev) (2.6.3)\n",
      "Requirement already satisfied: pandas~=1.3.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from openvino-dev) (1.3.5)\n",
      "Requirement already satisfied: tqdm>=4.54.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from openvino-dev) (4.62.3)\n",
      "Requirement already satisfied: scipy>=1.8 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from openvino-dev) (1.9.3)\n",
      "Requirement already satisfied: pyyaml>=5.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from openvino-dev) (5.4.1)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from openvino-dev) (1.23.4)\n",
      "Requirement already satisfied: addict>=2.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from openvino-dev) (2.4.0)\n",
      "Requirement already satisfied: openvino-telemetry>=2022.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from openvino-dev) (2022.3.0)\n",
      "Requirement already satisfied: requests>=2.25.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from openvino-dev) (2.26.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pandas~=1.3.5->openvino-dev) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pandas~=1.3.5->openvino-dev) (2021.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests>=2.25.1->openvino-dev) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests>=2.25.1->openvino-dev) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests>=2.25.1->openvino-dev) (3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests>=2.25.1->openvino-dev) (2.0.7)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas~=1.3.5->openvino-dev) (1.16.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p38/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: openvino-dev[onnx] in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (2022.3.0)\n",
      "Requirement already satisfied: scipy>=1.8 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from openvino-dev[onnx]) (1.9.3)\n",
      "Requirement already satisfied: pandas~=1.3.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from openvino-dev[onnx]) (1.3.5)\n",
      "Requirement already satisfied: texttable>=1.6.3 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from openvino-dev[onnx]) (1.6.7)\n",
      "Requirement already satisfied: tqdm>=4.54.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from openvino-dev[onnx]) (4.62.3)\n",
      "Requirement already satisfied: openvino-telemetry>=2022.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from openvino-dev[onnx]) (2022.3.0)\n",
      "Requirement already satisfied: requests>=2.25.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from openvino-dev[onnx]) (2.26.0)\n",
      "Requirement already satisfied: opencv-python>=4.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from openvino-dev[onnx]) (4.5.1.48)\n",
      "Requirement already satisfied: pillow>=8.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from openvino-dev[onnx]) (9.0.1)\n",
      "Requirement already satisfied: openvino==2022.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from openvino-dev[onnx]) (2022.3.0)\n",
      "Requirement already satisfied: networkx<=2.8.8 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from openvino-dev[onnx]) (2.6.3)\n",
      "Requirement already satisfied: jstyleson>=0.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from openvino-dev[onnx]) (0.0.2)\n",
      "Requirement already satisfied: numpy<=1.23.4,>=1.16.6 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from openvino-dev[onnx]) (1.23.4)\n",
      "Requirement already satisfied: pyyaml>=5.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from openvino-dev[onnx]) (5.4.1)\n",
      "Requirement already satisfied: defusedxml>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from openvino-dev[onnx]) (0.7.1)\n",
      "Requirement already satisfied: addict>=2.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from openvino-dev[onnx]) (2.4.0)\n",
      "Requirement already satisfied: onnx<=1.12,>=1.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from openvino-dev[onnx]) (1.10.2)\n",
      "Requirement already satisfied: protobuf<4.0.0,>=3.18.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from openvino-dev[onnx]) (3.19.4)\n",
      "Collecting fastjsonschema~=2.15.1\n",
      "  Downloading fastjsonschema-2.15.3-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from onnx<=1.12,>=1.8.1->openvino-dev[onnx]) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from onnx<=1.12,>=1.8.1->openvino-dev[onnx]) (4.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pandas~=1.3.5->openvino-dev[onnx]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pandas~=1.3.5->openvino-dev[onnx]) (2021.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests>=2.25.1->openvino-dev[onnx]) (3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests>=2.25.1->openvino-dev[onnx]) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests>=2.25.1->openvino-dev[onnx]) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests>=2.25.1->openvino-dev[onnx]) (1.26.8)\n",
      "Installing collected packages: fastjsonschema\n",
      "Successfully installed fastjsonschema-2.15.3\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p38/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install openvino-dev\n",
    "!pip install openvino-dev[onnx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: main.py [options]\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --framework {paddle,caffe,kaldi,mxnet,onnx,tf}\n",
      "                        Name of the framework used to train the input model.\n",
      "\n",
      "Framework-agnostic parameters:\n",
      "  --input_model INPUT_MODEL, -w INPUT_MODEL, -m INPUT_MODEL\n",
      "                        {} Tensorflow*: a file with a pre-trained model\n",
      "                        (binary or text .pb file after freezing). Caffe*: a\n",
      "                        model proto file with model weights\n",
      "  --model_name MODEL_NAME, -n MODEL_NAME\n",
      "                        Model_name parameter passed to the final create_ir\n",
      "                        transform. This parameter is used to name a network in\n",
      "                        a generated IR and output .xml/.bin files.\n",
      "  --output_dir OUTPUT_DIR, -o OUTPUT_DIR\n",
      "                        Directory that stores the generated IR. By default, it\n",
      "                        is the directory from where the Model Optimizer is\n",
      "                        launched.\n",
      "  --input_shape INPUT_SHAPE\n",
      "                        Input shape(s) that should be fed to an input node(s)\n",
      "                        of the model. Shape is defined as a comma-separated\n",
      "                        list of integer numbers enclosed in parentheses or\n",
      "                        square brackets, for example [1,3,227,227] or\n",
      "                        (1,227,227,3), where the order of dimensions depends\n",
      "                        on the framework input layout of the model. For\n",
      "                        example, [N,C,H,W] is used for ONNX* models and\n",
      "                        [N,H,W,C] for TensorFlow* models. The shape can\n",
      "                        contain undefined dimensions (? or -1) and should fit\n",
      "                        the dimensions defined in the input operation of the\n",
      "                        graph. Boundaries of undefined dimension can be\n",
      "                        specified with ellipsis, for example\n",
      "                        [1,1..10,128,128]. One boundary can be undefined, for\n",
      "                        example [1,..100] or [1,3,1..,1..]. If there are\n",
      "                        multiple inputs in the model, --input_shape should\n",
      "                        contain definition of shape for each input separated\n",
      "                        by a comma, for example: [1,3,227,227],[2,4] for a\n",
      "                        model with two inputs with 4D and 2D shapes.\n",
      "                        Alternatively, specify shapes with the --input option.\n",
      "  --scale SCALE, -s SCALE\n",
      "                        All input values coming from original network inputs\n",
      "                        will be divided by this value. When a list of inputs\n",
      "                        is overridden by the --input parameter, this scale is\n",
      "                        not applied for any input that does not match with the\n",
      "                        original input of the model.If both --mean_values and\n",
      "                        --scale are specified, the mean is subtracted first\n",
      "                        and then scale is applied regardless of the order of\n",
      "                        options in command line.\n",
      "  --reverse_input_channels\n",
      "                        Switch the input channels order from RGB to BGR (or\n",
      "                        vice versa). Applied to original inputs of the model\n",
      "                        if and only if a number of channels equals 3. When\n",
      "                        --mean_values/--scale_values are also specified,\n",
      "                        reversing of channels will be applied to user's input\n",
      "                        data first, so that numbers in --mean_values and\n",
      "                        --scale_values go in the order of channels used in the\n",
      "                        original model. In other words, if both options are\n",
      "                        specified, then the data flow in the model looks as\n",
      "                        following: Parameter -> ReverseInputChannels -> Mean\n",
      "                        apply-> Scale apply -> the original body of the model.\n",
      "  --log_level {CRITICAL,ERROR,WARN,WARNING,INFO,DEBUG,NOTSET}\n",
      "                        Logger level\n",
      "  --input INPUT         Quoted list of comma-separated input nodes names with\n",
      "                        shapes, data types, and values for freezing. The order\n",
      "                        of inputs in converted model is the same as order of\n",
      "                        specified operation names. The shape and value are\n",
      "                        specified as comma-separated lists. The data type of\n",
      "                        input node is specified in braces and can have one of\n",
      "                        the values: f64 (float64), f32 (float32), f16\n",
      "                        (float16), i64 (int64), i32 (int32), u8 (uint8),\n",
      "                        boolean (bool). Data type is optional. If it's not\n",
      "                        specified explicitly then there are two options: if\n",
      "                        input node is a parameter, data type is taken from the\n",
      "                        original node dtype, if input node is not a parameter,\n",
      "                        data type is set to f32. Example, to set `input_1`\n",
      "                        with shape [1,100], and Parameter node `sequence_len`\n",
      "                        with scalar input with value `150`, and boolean input\n",
      "                        `is_training` with `False` value use the following\n",
      "                        format:\n",
      "                        \"input_1[1,100],sequence_len->150,is_training->False\".\n",
      "                        Another example, use the following format to set input\n",
      "                        port 0 of the node `node_name1` with the shape [3,4]\n",
      "                        as an input node and freeze output port 1 of the node\n",
      "                        `node_name2` with the value [20,15] of the int32 type\n",
      "                        and shape [2]:\n",
      "                        \"0:node_name1[3,4],node_name2:1[2]{i32}->[20,15]\".\n",
      "  --output OUTPUT       The name of the output operation of the model or list\n",
      "                        of names. For TensorFlow*, do not add :0 to this\n",
      "                        name.The order of outputs in converted model is the\n",
      "                        same as order of specified operation names.\n",
      "  --mean_values MEAN_VALUES, -ms MEAN_VALUES\n",
      "                        Mean values to be used for the input image per\n",
      "                        channel. Values to be provided in the (R,G,B) or\n",
      "                        [R,G,B] format. Can be defined for desired input of\n",
      "                        the model, for example: \"--mean_values\n",
      "                        data[255,255,255],info[255,255,255]\". The exact\n",
      "                        meaning and order of channels depend on how the\n",
      "                        original model was trained.\n",
      "  --scale_values SCALE_VALUES\n",
      "                        Scale values to be used for the input image per\n",
      "                        channel. Values are provided in the (R,G,B) or [R,G,B]\n",
      "                        format. Can be defined for desired input of the model,\n",
      "                        for example: \"--scale_values\n",
      "                        data[255,255,255],info[255,255,255]\". The exact\n",
      "                        meaning and order of channels depend on how the\n",
      "                        original model was trained.If both --mean_values and\n",
      "                        --scale_values are specified, the mean is subtracted\n",
      "                        first and then scale is applied regardless of the\n",
      "                        order of options in command line.\n",
      "  --source_layout SOURCE_LAYOUT\n",
      "                        Layout of the input or output of the model in the\n",
      "                        framework. Layout can be specified in the short form,\n",
      "                        e.g. nhwc, or in complex form, e.g. \"[n,h,w,c]\".\n",
      "                        Example for many names: \"in_name1([n,h,w,c]),in_name2(\n",
      "                        nc),out_name1(n),out_name2(nc)\". Layout can be\n",
      "                        partially defined, \"?\" can be used to specify\n",
      "                        undefined layout for one dimension, \"...\" can be used\n",
      "                        to specify undefined layout for multiple dimensions,\n",
      "                        for example \"?c??\", \"nc...\", \"n...c\", etc.\n",
      "  --target_layout TARGET_LAYOUT\n",
      "                        Same as --source_layout, but specifies target layout\n",
      "                        that will be in the model after processing by\n",
      "                        ModelOptimizer.\n",
      "  --layout LAYOUT       Combination of --source_layout and --target_layout.\n",
      "                        Can't be used with either of them. If model has one\n",
      "                        input it is sufficient to specify layout of this\n",
      "                        input, for example --layout nhwc. To specify layouts\n",
      "                        of many tensors, names must be provided, for example:\n",
      "                        --layout \"name1(nchw),name2(nc)\". It is possible to\n",
      "                        instruct ModelOptimizer to change layout, for example:\n",
      "                        --layout \"name1(nhwc->nchw),name2(cn->nc)\". Also \"*\"\n",
      "                        in long layout form can be used to fuse dimensions,\n",
      "                        for example \"[n,c,...]->[n*c,...]\".\n",
      "  --data_type {FP16,FP32,half,float}\n",
      "                        [DEPRECATED] Data type for model weights and biases.\n",
      "                        If original model has FP32 weights or biases and\n",
      "                        --data_type=FP16 is specified, FP32 model weights and\n",
      "                        biases are compressed to FP16. All intermediate data\n",
      "                        is kept in original precision.\n",
      "  --compress_to_fp16 [COMPRESS_TO_FP16]\n",
      "                        If the original model has FP32 weights or biases, they\n",
      "                        are compressed to FP16. All intermediate data is kept\n",
      "                        in original precision.\n",
      "  --transform TRANSFORM\n",
      "                        Apply additional transformations. Usage: \"--transform\n",
      "                        transformation_name1[args],transformation_name2...\"\n",
      "                        where [args] is key=value pairs separated by\n",
      "                        semicolon. Examples: \"--transform LowLatency2\" or \"--\n",
      "                        transform Pruning\" or \"--transform\n",
      "                        LowLatency2[use_const_initializer=False]\" or \"--\n",
      "                        transform \"MakeStateful[param_res_names= {'input_name_\n",
      "                        1':'output_name_1','input_name_2':'output_name_2'}]\"\"\n",
      "                        Available transformations: \"LowLatency2\",\n",
      "                        \"MakeStateful\", \"Pruning\"\n",
      "  --disable_fusing      [DEPRECATED] Turn off fusing of linear operations to\n",
      "                        Convolution.\n",
      "  --disable_resnet_optimization\n",
      "                        [DEPRECATED] Turn off ResNet optimization.\n",
      "  --finegrain_fusing FINEGRAIN_FUSING\n",
      "                        [DEPRECATED] Regex for layers/operations that won't be\n",
      "                        fused. Example: --finegrain_fusing\n",
      "                        Convolution1,.*Scale.*\n",
      "  --enable_concat_optimization\n",
      "                        [DEPRECATED] Turn on Concat optimization.\n",
      "  --extensions EXTENSIONS\n",
      "                        Paths or a comma-separated list of paths to libraries\n",
      "                        (.so or .dll) with extensions. For the legacy MO path\n",
      "                        (if `--use_legacy_frontend` is used), a directory or a\n",
      "                        comma-separated list of directories with extensions\n",
      "                        are supported. To disable all extensions including\n",
      "                        those that are placed at the default location, pass an\n",
      "                        empty string.\n",
      "  --batch BATCH, -b BATCH\n",
      "                        Input batch size\n",
      "  --version             Version of Model Optimizer\n",
      "  --silent SILENT       Prevent any output messages except those that\n",
      "                        correspond to log level equals ERROR, that can be set\n",
      "                        with the following option: --log_level. By default,\n",
      "                        log level is already ERROR.\n",
      "  --freeze_placeholder_with_value FREEZE_PLACEHOLDER_WITH_VALUE\n",
      "                        Replaces input layer with constant node with provided\n",
      "                        value, for example: \"node_name->True\". It will be\n",
      "                        DEPRECATED in future releases. Use --input option to\n",
      "                        specify a value for freezing.\n",
      "  --static_shape        Enables IR generation for fixed input shape (folding\n",
      "                        `ShapeOf` operations and shape-calculating sub-graphs\n",
      "                        to `Constant`). Changing model input shape using the\n",
      "                        OpenVINO Runtime API in runtime may fail for such an\n",
      "                        IR.\n",
      "  --disable_weights_compression\n",
      "                        [DEPRECATED] Disable compression and store weights\n",
      "                        with original precision.\n",
      "  --progress            Enable model conversion progress display.\n",
      "  --stream_output       Switch model conversion progress display to a\n",
      "                        multiline mode.\n",
      "  --transformations_config TRANSFORMATIONS_CONFIG\n",
      "                        Use the configuration file with transformations\n",
      "                        description. Transformations file can be specified as\n",
      "                        relative path from the current directory, as absolute\n",
      "                        path or as arelative path from the mo root directory.\n",
      "  --use_new_frontend    Force the usage of new Frontend of Model Optimizer for\n",
      "                        model conversion into IR. The new Frontend is C++\n",
      "                        based and is available for ONNX* and PaddlePaddle*\n",
      "                        models. Model optimizer uses new Frontend for ONNX*\n",
      "                        and PaddlePaddle* by default that means\n",
      "                        `--use_new_frontend` and `--use_legacy_frontend`\n",
      "                        options are not specified.\n",
      "  --use_legacy_frontend\n",
      "                        Force the usage of legacy Frontend of Model Optimizer\n",
      "                        for model conversion into IR. The legacy Frontend is\n",
      "                        Python based and is available for TensorFlow*, ONNX*,\n",
      "                        MXNet*, Caffe*, and Kaldi* models.\n",
      "\n",
      "TensorFlow*-specific parameters:\n",
      "  --input_model_is_text\n",
      "                        TensorFlow*: treat the input model file as a text\n",
      "                        protobuf format. If not specified, the Model Optimizer\n",
      "                        treats it as a binary file by default.\n",
      "  --input_checkpoint INPUT_CHECKPOINT\n",
      "                        TensorFlow*: variables file to load.\n",
      "  --input_meta_graph INPUT_META_GRAPH\n",
      "                        Tensorflow*: a file with a meta-graph of the model\n",
      "                        before freezing\n",
      "  --saved_model_dir SAVED_MODEL_DIR\n",
      "                        TensorFlow*: directory with a model in SavedModel\n",
      "                        format of TensorFlow 1.x or 2.x version.\n",
      "  --saved_model_tags SAVED_MODEL_TAGS\n",
      "                        Group of tag(s) of the MetaGraphDef to load, in string\n",
      "                        format, separated by ','. For tag-set contains\n",
      "                        multiple tags, all tags must be passed in.\n",
      "  --tensorflow_custom_operations_config_update TENSORFLOW_CUSTOM_OPERATIONS_CONFIG_UPDATE\n",
      "                        TensorFlow*: update the configuration file with node\n",
      "                        name patterns with input/output nodes information.\n",
      "  --tensorflow_use_custom_operations_config TENSORFLOW_USE_CUSTOM_OPERATIONS_CONFIG\n",
      "                        Use the configuration file with custom operation\n",
      "                        description.\n",
      "  --tensorflow_object_detection_api_pipeline_config TENSORFLOW_OBJECT_DETECTION_API_PIPELINE_CONFIG\n",
      "                        TensorFlow*: path to the pipeline configuration file\n",
      "                        used to generate model created with help of Object\n",
      "                        Detection API.\n",
      "  --tensorboard_logdir TENSORBOARD_LOGDIR\n",
      "                        TensorFlow*: dump the input graph to a given directory\n",
      "                        that should be used with TensorBoard.\n",
      "  --tensorflow_custom_layer_libraries TENSORFLOW_CUSTOM_LAYER_LIBRARIES\n",
      "                        TensorFlow*: comma separated list of shared libraries\n",
      "                        with TensorFlow* custom operations implementation.\n",
      "  --disable_nhwc_to_nchw\n",
      "                        [DEPRECATED] Disables the default translation from\n",
      "                        NHWC to NCHW. Since 2022.1 this option is deprecated\n",
      "                        and used only to maintain backward compatibility with\n",
      "                        previous releases.\n",
      "\n",
      "Caffe*-specific parameters:\n",
      "  --input_proto INPUT_PROTO, -d INPUT_PROTO\n",
      "                        Deploy-ready prototxt file that contains a topology\n",
      "                        structure and layer attributes\n",
      "  --caffe_parser_path CAFFE_PARSER_PATH\n",
      "                        Path to Python Caffe* parser generated from\n",
      "                        caffe.proto\n",
      "  -k K                  Path to CustomLayersMapping.xml to register custom\n",
      "                        layers\n",
      "  --mean_file MEAN_FILE, -mf MEAN_FILE\n",
      "                        [DEPRECATED] Mean image to be used for the input.\n",
      "                        Should be a binaryproto file\n",
      "  --mean_file_offsets MEAN_FILE_OFFSETS, -mo MEAN_FILE_OFFSETS\n",
      "                        [DEPRECATED] Mean image offsets to be used for the\n",
      "                        input binaryproto file. When the mean image is bigger\n",
      "                        than the expected input, it is cropped. By default,\n",
      "                        centers of the input image and the mean image are the\n",
      "                        same and the mean image is cropped by dimensions of\n",
      "                        the input image. The format to pass this option is the\n",
      "                        following: \"-mo (x,y)\". In this case, the mean file is\n",
      "                        cropped by dimensions of the input image with offset\n",
      "                        (x,y) from the upper left corner of the mean image\n",
      "  --disable_omitting_optional\n",
      "                        Disable omitting optional attributes to be used for\n",
      "                        custom layers. Use this option if you want to transfer\n",
      "                        all attributes of a custom layer to IR. Default\n",
      "                        behavior is to transfer the attributes with default\n",
      "                        values and the attributes defined by the user to IR.\n",
      "  --enable_flattening_nested_params\n",
      "                        Enable flattening optional params to be used for\n",
      "                        custom layers. Use this option if you want to transfer\n",
      "                        attributes of a custom layer to IR with flattened\n",
      "                        nested parameters. Default behavior is to transfer the\n",
      "                        attributes without flattening nested parameters.\n",
      "\n",
      "Mxnet-specific parameters:\n",
      "  --input_symbol INPUT_SYMBOL\n",
      "                        Symbol file (for example, model-symbol.json) that\n",
      "                        contains a topology structure and layer attributes\n",
      "  --nd_prefix_name ND_PREFIX_NAME\n",
      "                        Prefix name for args.nd and argx.nd files.\n",
      "  --pretrained_model_name PRETRAINED_MODEL_NAME\n",
      "                        Name of a pretrained MXNet model without extension and\n",
      "                        epoch number. This model will be merged with args.nd\n",
      "                        and argx.nd files\n",
      "  --save_params_from_nd\n",
      "                        Enable saving built parameters file from .nd files\n",
      "  --legacy_mxnet_model  Enable MXNet loader to make a model compatible with\n",
      "                        the latest MXNet version. Use only if your model was\n",
      "                        trained with MXNet version lower than 1.0.0\n",
      "  --enable_ssd_gluoncv  Enable pattern matchers replacers for converting\n",
      "                        gluoncv ssd topologies.\n",
      "\n",
      "Kaldi-specific parameters:\n",
      "  --counts COUNTS       Path to the counts file\n",
      "  --remove_output_softmax\n",
      "                        Removes the SoftMax layer that is the output layer\n",
      "  --remove_memory       Removes the Memory layer and use additional inputs\n",
      "                        outputs instead\n"
     ]
    }
   ],
   "source": [
    "! mo -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Optimizer command to convert the ONNX model to OpenVINO:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "`mo --input_model \"tinynet.onnx\" --input_shape \"[1,3, 512, 512]\" --data_type FP32 --output_dir \"irmodel\"`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Construct the command for Model Optimizer.\n",
    "from pathlib import Path\n",
    "onnx_path=\"tinynet.onnx\"\n",
    "model_path=\"irmodel\"\n",
    "model_path = Path(model_path)\n",
    "ir_path = model_path.with_suffix(\".xml\")\n",
    "IMAGE_HEIGHT=512\n",
    "IMAGE_WIDTH=512\n",
    "mo_command = f\"\"\"mo\n",
    "                 --input_model \"{onnx_path}\"\n",
    "                 --input_shape \"[1,3, {IMAGE_HEIGHT}, {IMAGE_WIDTH}]\"\n",
    "                 --data_type FP32\n",
    "                 --output_dir \"{model_path}\"\n",
    "                 \"\"\"\n",
    "mo_command = \" \".join(mo_command.split())\n",
    "print(\"Model Optimizer command to convert the ONNX model to OpenVINO:\")\n",
    "display(Markdown(f\"`{mo_command}`\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting ONNX model to IR... This may take a few minutes.\n",
      "[ WARNING ]  Use of deprecated cli option --data_type detected. Option use in the following releases will be fatal. \n",
      "[ INFO ] The model was converted to IR v11, the latest model format that corresponds to the source DL framework input/output format. While IR v11 is backwards compatible with OpenVINO Inference Engine API v1.0, please use API v2.0 (as of 2022.1) to take advantage of the latest improvements in IR v11.\n",
      "Find more information about API v2.0 and IR v11 at https://docs.openvino.ai/latest/openvino_2_0_transition_guide.html\n",
      "[ SUCCESS ] Generated IR version 11 model.\n",
      "[ SUCCESS ] XML file: /home/ec2-user/SageMaker/tiny-instance-segmentation/notebooks/irmodel/tinynet.xml\n",
      "[ SUCCESS ] BIN file: /home/ec2-user/SageMaker/tiny-instance-segmentation/notebooks/irmodel/tinynet.bin\n"
     ]
    }
   ],
   "source": [
    "print(\"Exporting ONNX model to IR... This may take a few minutes.\")\n",
    "mo_result = %sx $mo_command\n",
    "print(\"\\n\".join(mo_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "ir_path=Path(\"irmodel/tinynet.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.runtime import Core\n",
    "ie = Core()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ir = ie.read_model(model=ir_path)\n",
    "compiled_model_ir = ie.compile_model(model=model_ir, device_name=\"CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<CompiledModel:\n",
       "inputs[\n",
       "<ConstOutput: names[input] shape[1,3,512,512] type: f32>\n",
       "]\n",
       "outputs[\n",
       "<ConstOutput: names[output] shape[..4096] type: i64>,\n",
       "<ConstOutput: names[872] shape[4,1,64,64] type: f32>\n",
       "]>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compiled_model_ir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ConstOutput: names[872] shape[4,1,64,64] type: f32>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get input and output layers.\n",
    "output_layer_ir = compiled_model_ir.output(1)\n",
    "output_layer_ir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1, 64, 64)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_ir = compiled_model_ir([input_image])[output_layer_ir]\n",
    "res_ir.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.1649176 , 0.16338456, 0.16040629, ..., 0.16071784,\n",
       "          0.16183224, 0.15289238],\n",
       "         [0.16723719, 0.17236677, 0.17437999, ..., 0.16114116,\n",
       "          0.16439743, 0.15792395],\n",
       "         [0.16422993, 0.16862865, 0.1739501 , ..., 0.16700855,\n",
       "          0.16054681, 0.15998141],\n",
       "         ...,\n",
       "         [0.13980986, 0.1518708 , 0.1780786 , ..., 0.15225607,\n",
       "          0.16712587, 0.17388214],\n",
       "         [0.13712543, 0.14206952, 0.1637606 , ..., 0.16094285,\n",
       "          0.15684302, 0.15327564],\n",
       "         [0.13647453, 0.14211504, 0.14452797, ..., 0.17303163,\n",
       "          0.18076348, 0.1686283 ]]],\n",
       "\n",
       "\n",
       "       [[[0.15835004, 0.15854935, 0.15511955, ..., 0.180603  ,\n",
       "          0.17175457, 0.1718836 ],\n",
       "         [0.1603279 , 0.16409267, 0.16513075, ..., 0.189782  ,\n",
       "          0.18537395, 0.17714027],\n",
       "         [0.16137466, 0.16503088, 0.17318162, ..., 0.1753896 ,\n",
       "          0.17245564, 0.17549357],\n",
       "         ...,\n",
       "         [0.13875452, 0.14308529, 0.14854528, ..., 0.14195853,\n",
       "          0.11302772, 0.1366423 ],\n",
       "         [0.1446408 , 0.13824084, 0.14130297, ..., 0.14757636,\n",
       "          0.12759794, 0.14785293],\n",
       "         [0.15976804, 0.14485724, 0.15188348, ..., 0.13562503,\n",
       "          0.1247028 , 0.14126396]]],\n",
       "\n",
       "\n",
       "       [[[0.15344284, 0.14963686, 0.14561057, ..., 0.16346204,\n",
       "          0.14478746, 0.14734925],\n",
       "         [0.15824337, 0.15853022, 0.14748812, ..., 0.16250171,\n",
       "          0.15401189, 0.15200515],\n",
       "         [0.15807687, 0.16055028, 0.16207634, ..., 0.15787365,\n",
       "          0.15474132, 0.16532078],\n",
       "         ...,\n",
       "         [0.12190366, 0.12320916, 0.13751677, ..., 0.1705347 ,\n",
       "          0.16419798, 0.1614289 ],\n",
       "         [0.13885482, 0.12917191, 0.13256049, ..., 0.17304353,\n",
       "          0.17083648, 0.15809357],\n",
       "         [0.1387587 , 0.13056223, 0.13350935, ..., 0.15894462,\n",
       "          0.15185377, 0.15516765]]],\n",
       "\n",
       "\n",
       "       [[[0.16505754, 0.16169524, 0.16166434, ..., 0.1395128 ,\n",
       "          0.12418664, 0.12090524],\n",
       "         [0.167332  , 0.16884199, 0.16816783, ..., 0.13731761,\n",
       "          0.13352203, 0.13365793],\n",
       "         [0.16197133, 0.16211884, 0.16350639, ..., 0.14211416,\n",
       "          0.13887441, 0.13817798],\n",
       "         ...,\n",
       "         [0.12350889, 0.1370066 , 0.15530074, ..., 0.9628268 ,\n",
       "          0.96360743, 0.91510934],\n",
       "         [0.12390599, 0.13189954, 0.14458825, ..., 0.9465247 ,\n",
       "          0.5640315 , 0.21548031],\n",
       "         [0.12914553, 0.13165411, 0.13227323, ..., 0.17156269,\n",
       "          0.15231375, 0.13580526]]]], dtype=float32)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_ir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.0000000e+00,  0.0000000e+00,  1.4901161e-08, ...,\n",
       "           0.0000000e+00, -1.4901161e-08,  2.9802322e-08],\n",
       "         [ 1.4901161e-08,  5.9604645e-08,  1.4901161e-08, ...,\n",
       "           0.0000000e+00, -2.9802322e-08, -1.4901161e-08],\n",
       "         [-1.4901161e-08,  4.4703484e-08, -1.4901161e-08, ...,\n",
       "           1.4901161e-08,  2.9802322e-08, -4.4703484e-08],\n",
       "         ...,\n",
       "         [-4.4703484e-08, -4.4703484e-08, -1.4901161e-08, ...,\n",
       "           7.4505806e-08,  1.4901161e-08,  8.9406967e-08],\n",
       "         [-4.4703484e-08,  0.0000000e+00, -7.4505806e-08, ...,\n",
       "           1.4901161e-08, -5.9604645e-08,  0.0000000e+00],\n",
       "         [ 0.0000000e+00, -4.4703484e-08, -7.4505806e-08, ...,\n",
       "          -4.4703484e-08, -1.4901161e-08,  0.0000000e+00]]],\n",
       "\n",
       "\n",
       "       [[[ 0.0000000e+00,  2.9802322e-08,  7.4505806e-08, ...,\n",
       "           2.9802322e-08,  1.4901161e-08, -2.9802322e-08],\n",
       "         [-2.9802322e-08,  0.0000000e+00,  1.4901161e-08, ...,\n",
       "           1.4901161e-08,  5.9604645e-08,  2.9802322e-08],\n",
       "         [-1.4901161e-08,  1.4901161e-08,  1.4901161e-08, ...,\n",
       "           1.4901161e-08,  2.9802322e-08, -1.4901161e-08],\n",
       "         ...,\n",
       "         [-1.4901161e-08,  1.4901161e-08, -1.4901161e-08, ...,\n",
       "          -1.4901161e-08,  4.4703484e-08, -1.1920929e-07],\n",
       "         [ 1.4901161e-08,  0.0000000e+00,  2.9802322e-08, ...,\n",
       "           0.0000000e+00,  5.9604645e-08, -8.9406967e-08],\n",
       "         [-2.9802322e-08, -5.9604645e-08, -5.9604645e-08, ...,\n",
       "          -2.9802322e-08,  2.9802322e-08, -4.4703484e-08]]],\n",
       "\n",
       "\n",
       "       [[[ 1.4901161e-08,  0.0000000e+00,  0.0000000e+00, ...,\n",
       "           0.0000000e+00, -1.4901161e-08,  0.0000000e+00],\n",
       "         [-1.4901161e-08, -4.4703484e-08,  1.4901161e-08, ...,\n",
       "           0.0000000e+00,  1.4901161e-08,  1.4901161e-08],\n",
       "         [-1.4901161e-08, -1.4901161e-08, -2.9802322e-08, ...,\n",
       "          -2.9802322e-08,  0.0000000e+00,  4.4703484e-08],\n",
       "         ...,\n",
       "         [ 2.9802322e-08,  0.0000000e+00,  2.9802322e-08, ...,\n",
       "           4.4703484e-08,  0.0000000e+00, -5.9604645e-08],\n",
       "         [ 1.4901161e-08,  0.0000000e+00,  2.9802322e-08, ...,\n",
       "           8.9406967e-08,  0.0000000e+00,  0.0000000e+00],\n",
       "         [-1.4901161e-08,  0.0000000e+00, -1.4901161e-08, ...,\n",
       "          -1.4901161e-08, -2.9802322e-08,  2.9802322e-08]]],\n",
       "\n",
       "\n",
       "       [[[-2.9802322e-08,  1.4901161e-08,  2.9802322e-08, ...,\n",
       "           0.0000000e+00,  1.4901161e-08,  0.0000000e+00],\n",
       "         [ 1.4901161e-08,  2.9802322e-08,  2.9802322e-08, ...,\n",
       "           5.9604645e-08,  0.0000000e+00,  1.4901161e-08],\n",
       "         [ 1.4901161e-08,  0.0000000e+00,  0.0000000e+00, ...,\n",
       "           1.4901161e-08,  0.0000000e+00,  1.4901161e-08],\n",
       "         ...,\n",
       "         [ 2.2351742e-08, -1.4901161e-08, -1.4901161e-08, ...,\n",
       "           0.0000000e+00, -5.9604645e-08, -5.9604645e-08],\n",
       "         [ 8.1956387e-08,  1.4901161e-08, -1.4901161e-08, ...,\n",
       "          -2.3841858e-07, -1.6093254e-06, -4.6193600e-07],\n",
       "         [ 1.4901161e-08,  1.4901161e-08,  0.0000000e+00, ...,\n",
       "          -8.9406967e-08, -1.4901161e-08, -1.4901161e-08]]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2-res_ir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(res_ir,y2,rtol=1e-04, atol=1e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Weights conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posencoding=True\n",
    "model_curr=tinyModel(posEncoding=posencoding,deploy=False)\n",
    "curr_state_dict=model_curr.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device(\"cpu\")\n",
    "loaded_state_dict=torch.load('../weights/RepVGGplus-L2pse-train256-acc84.06.pth',map_location=device)\n",
    "new_state_dict={}\n",
    "matched_layers=0\n",
    "unmatched_layers=0\n",
    "for k in curr_state_dict.keys():\n",
    "    if \"backbone\" in k:\n",
    "        modk=k.split(\".\")[1:]\n",
    "        modk=\".\".join(modk)\n",
    "        #print(f\"mod k is {modk}\")\n",
    "    else:\n",
    "        modk=k\n",
    "    if modk in loaded_state_dict.keys() and loaded_state_dict[modk].size()==curr_state_dict[k].size():\n",
    "        new_state_dict[k]=loaded_state_dict[modk]\n",
    "        matched_layers+=1\n",
    "    else:\n",
    "        new_state_dict[k]=curr_state_dict[k]\n",
    "        unmatched_layers+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## find any layer of same size\n",
    "device=torch.device(\"cpu\")\n",
    "loaded_state_dict=torch.load('../weights/RepVGGplus-L2pse-train256-acc84.06.pth',map_location=device)\n",
    "new_state_dict={}\n",
    "matched_layers=0\n",
    "unmatched_layers=0\n",
    "for k in curr_state_dict.keys():\n",
    "    findsz=curr_state_dict[k].size()\n",
    "    found=False\n",
    "    for kl in loaded_state_dict.keys():\n",
    "        foundsz=loaded_state_dict[kl].size()\n",
    "        if findsz==foundsz:\n",
    "            new_state_dict[k]=loaded_state_dict[kl]\n",
    "            matched_layers+=1\n",
    "            found=True\n",
    "    if not found:\n",
    "        new_state_dict[k]=curr_state_dict[k]\n",
    "        unmatched_layers+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Matched Layers {matched_layers}\")\n",
    "print(f\"Unmatched Layers {unmatched_layers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#curr_state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(new_state_dict, \"../weights/repvgg_pretrain.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load repvggplus\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('../models'))\n",
    "from repvggplus_tinysim import tinyModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posencoding=True\n",
    "model_curr=tinyModel(posEncoding=posencoding,deploy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device(\"cpu\")\n",
    "loaded_state_dict=torch.load('../weights/RepVGGplus-L2pse-train256-acc84.06.pth',map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_curr.backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_curr.backbone.load_state_dict(loaded_state_dict,strict=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('pytorch_p38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "591ade058296000ef487104c42d37478df75296e039f312fe940d4accf2fd7b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
